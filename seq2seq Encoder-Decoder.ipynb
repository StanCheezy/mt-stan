{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB3RXcYyL9gQ"
   },
   "source": [
    "# Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lh65nfALNlIH"
   },
   "source": [
    "### Dataset Preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KF477bqbGeNu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fU80Ao-AGaob"
   },
   "outputs": [],
   "source": [
    "file = open(\"nya.tsv\", 'r', encoding = \"utf8\")\n",
    "raw_data = []\n",
    "\n",
    "for line in file:\n",
    "    pos = line.find(\"CC-BY\")\n",
    "    line = line[:pos-1]\n",
    "    \n",
    "    # Split the data into english and Italian\n",
    "    eng, nya = line.split('\\t')\n",
    "    \n",
    "    # form tuples of the data\n",
    "    data = eng, nya\n",
    "    raw_data.append(data)\n",
    "    \n",
    "file.close()\n",
    "\n",
    "def convert(list): \n",
    "    return tuple(list) \n",
    "  \n",
    "data = convert(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "kU5L1oaHIc5R"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(s):\n",
    "    s = unicode_to_ascii(s.lower())\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "\n",
    "    s = s.strip()\n",
    "    s = '<start>' +' '+ s +' '+' <end>'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ca73UuX8IdAR"
   },
   "outputs": [],
   "source": [
    "# Limiting the data and Splitting into seperate lists and add tokens\n",
    "\n",
    "data = data[:27000]\n",
    "\n",
    "lang_eng = []\n",
    "lang_nya = []\n",
    "\n",
    "raw_data_en, raw_data_nya = list(zip(*data))\n",
    "raw_data_en, raw_data_nya = list(raw_data_en), list(raw_data_nya)\n",
    "\n",
    "for i, j in zip(raw_data_en, raw_data_nya):\n",
    "  preprocessed_data_en = preprocess_sentence(i)\n",
    "  preprocessed_data_nya = preprocess_sentence(j)\n",
    "  lang_eng.append(preprocessed_data_en)\n",
    "  lang_nya.append(preprocessed_data_nya)\n",
    "\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "input_tensor, inp_lang = tokenize(lang_nya)\n",
    "target_tensor, targ_lang = tokenize(lang_eng)\n",
    "\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVfnBuJIIdC_",
    "outputId": "3e0d1cf1-fefe-4740-9fff-18422a4da1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 162 41 41\n",
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "4 ----> to\n",
      "77 ----> agree\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "31 ----> bvomera\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t!=0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "\n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eEha39YYFXn4",
    "outputId": "98456fac-e1f8-4c1a-9c5e-bed0fb212f4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 14:03:59.468543: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-19 14:03:59.468979: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 12), dtype=tf.int32, name=None), TensorSpec(shape=(64, 5), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB0m23oZSRxa"
   },
   "source": [
    "### Encoder Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KWHj4HbGIdFX"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, inp_vocab_size, embedding_size, lstm_size, input_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        #Initialize Embedding layer\n",
    "        #Intialize Encoder LSTM layer\n",
    "        \n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = tf.keras.layers.Embedding(inp_vocab_size, embedding_size)\n",
    "        self.lstm = tf.keras.layers.LSTM(lstm_size, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, input_sequence, states):\n",
    "      \n",
    "        embed = self.embedding(input_sequence)\n",
    "        output, state_h, state_c = self.lstm(embed, initial_state=states)\n",
    "\n",
    "        return output, state_h, state_c\n",
    "    \n",
    "    def initialize_states(self,batch_size):\n",
    "    \n",
    "        return (tf.zeros([batch_size, self.lstm_size]),\n",
    "                tf.zeros([batch_size, self.lstm_size]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sjb3_4c6Sld4"
   },
   "source": [
    "### Dot Attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oCaMyZWKSe_I"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,scoring_function, att_units):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.scoring_function = scoring_function\n",
    "        self.att_units = att_units\n",
    "\n",
    "        if self.scoring_function=='dot':\n",
    "            pass\n",
    "            # For general, it would be self.wa = tf.keras.layers.Dense(att_units)\n",
    "\n",
    "\n",
    "    def call(self,decoder_hidden_state,encoder_output):\n",
    "\n",
    "        if self.scoring_function == 'dot':\n",
    "            \n",
    "            new_state = tf.expand_dims(decoder_hidden_state, -1)\n",
    "            score = tf.matmul(encoder_output, new_state)\n",
    "            weights = tf.nn.softmax(score, axis=1)\n",
    "            context = weights * encoder_output\n",
    "            context_vector = tf.reduce_sum(context, axis=1)\n",
    "                                \n",
    "            return context_vector, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXFVBTXDS_-Y"
   },
   "source": [
    "### One Step Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "29ycCzwaS9ZZ"
   },
   "outputs": [],
   "source": [
    "class One_Step_Decoder(tf.keras.Model):\n",
    "    def __init__(self, tar_vocab_size, embedding_dim, input_length, dec_units, score_fun, att_units):\n",
    "        super(One_Step_Decoder, self).__init__()\n",
    "        # Initialize decoder embedding layer, LSTM and any other objects needed\n",
    "        self.tar_vocab_size = tar_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.embedding = tf.keras.layers.Embedding(self.tar_vocab_size, self.embedding_dim, \n",
    "                                                   input_length=self.input_length)\n",
    "        \n",
    "        self.lstm = tf.keras.layers.LSTM(self.dec_units, return_sequences=True, \n",
    "                                         return_state=True)\n",
    "        \n",
    "        self.output_layer = tf.keras.layers.Dense(self.tar_vocab_size)\n",
    "        \n",
    "        self.attention = Attention(self.score_fun, self.att_units)\n",
    "\n",
    "    def call(self, input_to_decoder, encoder_output, state_h, state_c):\n",
    "        \n",
    "        result = self.embedding(input_to_decoder)\n",
    "        \n",
    "        context_vector, weights = self.attention(state_h, encoder_output)\n",
    "        \n",
    "        concat = tf.concat([tf.expand_dims(context_vector, 1), result], axis=-1)\n",
    "        \n",
    "        decoder_output, hidden_state, cell_state = self.lstm(concat, initial_state=[state_h, state_c])\n",
    "        \n",
    "        final_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))\n",
    "        final_output = self.output_layer(final_output)\n",
    "        \n",
    "        return final_output, hidden_state, cell_state, weights, context_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LP7VnWvwTLpz"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GwF7tSNlTMy_"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, out_vocab_size, embedding_dim, output_length, dec_units ,score_fun ,att_units):\n",
    "        #Intialize necessary variables and create an object from the class onestepdecoder\n",
    "        super(Decoder, self).__init__()\n",
    "        self.out_vocab_size = out_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_length = output_length\n",
    "        self.dec_units = dec_units\n",
    "        self.score_fun = score_fun\n",
    "        self.att_units = att_units\n",
    "        self.onestepdecoder = One_Step_Decoder(self.out_vocab_size, self.embedding_dim, self.output_length,\n",
    "                                               self.dec_units, self.score_fun, self.att_units)\n",
    "        \n",
    "    def call(self, input_to_decoder,encoder_output,decoder_hidden_state,decoder_cell_state):\n",
    "        \n",
    "        all_outputs= tf.TensorArray(tf.float32, size=input_to_decoder.shape[1], name=\"output_arrays\")\n",
    "        \n",
    "        \n",
    "        for timestep in range(input_to_decoder.shape[1]):\n",
    "            output, decoder_hidden_state, decoder_cell_state, weights, context_vector = self.onestepdecoder(\n",
    "                                                                                    input_to_decoder[:,timestep:timestep+1], \n",
    "                                                                                    encoder_output, \n",
    "                                                                                    decoder_hidden_state,\n",
    "                                                                                    decoder_cell_state)\n",
    "            \n",
    "            all_outputs = all_outputs.write(timestep, output)\n",
    "        \n",
    "        all_outputs = tf.transpose(all_outputs.stack(), (1, 0, 2)) \n",
    "\n",
    "        return all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8OAoXiaITo07"
   },
   "source": [
    "### Call The Encoder Decoder Architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Qkg5csDhTW8Z"
   },
   "outputs": [],
   "source": [
    "class encoder_decoder(tf.keras.Model):\n",
    "    def __init__(self, inp_vocab_size, out_vocab_size, embedding_size, lstm_size, \n",
    "                 input_length, output_length, dec_units ,score_fun ,att_units, batch_size):\n",
    "        \n",
    "        super(encoder_decoder, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(inp_vocab_size, embedding_size, lstm_size, input_length)\n",
    "        self.decoder = Decoder(out_vocab_size, embedding_size, output_length, \n",
    "                               dec_units, score_fun, att_units)\n",
    "    \n",
    "    def call(self, data):\n",
    "        \n",
    "        input_sequence, input_to_decoder = data[0],data[1]\n",
    "        initial_state = self.encoder.initialize_states(batch_size=64)\n",
    "        encoder_output, state_h, state_c = self.encoder(input_sequence, initial_state)\n",
    "        decoder_hidden_state = state_h\n",
    "        decoder_cell_state = state_c\n",
    "        decoder_output = self.decoder(input_to_decoder, encoder_output, decoder_hidden_state, decoder_cell_state)\n",
    "        \n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4r8gqMeaT4DY"
   },
   "source": [
    "### Custom Loss Function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "A_uEicf9T2O6"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2bqE3wVUtOZ"
   },
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DB5pZyEFUwjl",
    "outputId": "1e3ffb1a-4c17-4834-eb75-de568d5b005c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: logs: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir logs\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"dot.h5\", monitor='val_loss', verbose=1, save_weights_only=True)\n",
    "\n",
    "logdir='logs'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)\n",
    "\n",
    "input_vocab_size = len(inp_lang.word_index)+1\n",
    "output_vocab_size = len(targ_lang.word_index)+1\n",
    "\n",
    "input_len = max_length_inp\n",
    "output_len = max_length_targ\n",
    "\n",
    "lstm_size = 128\n",
    "att_units = 256\n",
    "dec_units = 128\n",
    "embedding_size = 300\n",
    "embedding_dim = 300\n",
    "score_fun = 'dot'\n",
    "steps = len(input_tensor)//64\n",
    "batch_size=64\n",
    "\n",
    "model = encoder_decoder(input_vocab_size,output_vocab_size,embedding_size,lstm_size,input_len,output_len,dec_units,score_fun,att_units, batch_size)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=model.layers[0],\n",
    "                                 decoder=model.layers[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "HySImJjAU8dh"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden,enc_state = model.layers[0](inp, enc_hidden)\n",
    "\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      predictions = model.layers[1](dec_input,enc_output,enc_hidden,enc_state)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = model.layers[0].trainable_variables + model.layers[1].trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEYjgVJ-U_b4",
    "outputId": "575349d5-7583-4791-d471-a1f25ff55ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.0029\n",
      "Epoch 1 Loss 0.0071\n",
      "Time taken for 1 epoch 0.23241329193115234 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0055\n",
      "Epoch 2 Loss 0.0071\n",
      "Time taken for 1 epoch 0.16886186599731445 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0033\n",
      "Epoch 3 Loss 0.0084\n",
      "Time taken for 1 epoch 0.09501528739929199 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0032\n",
      "Epoch 4 Loss 0.0067\n",
      "Time taken for 1 epoch 0.16767191886901855 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0118\n",
      "Epoch 5 Loss 0.0084\n",
      "Time taken for 1 epoch 0.08693408966064453 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0110\n",
      "Epoch 6 Loss 0.0071\n",
      "Time taken for 1 epoch 0.18056797981262207 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0083\n",
      "Epoch 7 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08929896354675293 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0096\n",
      "Epoch 8 Loss 0.0080\n",
      "Time taken for 1 epoch 0.1609790325164795 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0048\n",
      "Epoch 9 Loss 0.0075\n",
      "Time taken for 1 epoch 0.08850502967834473 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0024\n",
      "Epoch 10 Loss 0.0062\n",
      "Time taken for 1 epoch 0.16075897216796875 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0045\n",
      "Epoch 11 Loss 0.0055\n",
      "Time taken for 1 epoch 0.08743596076965332 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0088\n",
      "Epoch 12 Loss 0.0084\n",
      "Time taken for 1 epoch 0.16933107376098633 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0049\n",
      "Epoch 13 Loss 0.0063\n",
      "Time taken for 1 epoch 0.08516287803649902 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0081\n",
      "Epoch 14 Loss 0.0062\n",
      "Time taken for 1 epoch 0.16481995582580566 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0094\n",
      "Epoch 15 Loss 0.0083\n",
      "Time taken for 1 epoch 0.0902860164642334 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0114\n",
      "Epoch 16 Loss 0.0070\n",
      "Time taken for 1 epoch 0.18370604515075684 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0049\n",
      "Epoch 17 Loss 0.0048\n",
      "Time taken for 1 epoch 0.09149694442749023 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0070\n",
      "Epoch 18 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16896796226501465 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0135\n",
      "Epoch 19 Loss 0.0082\n",
      "Time taken for 1 epoch 0.09307169914245605 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0069\n",
      "Epoch 20 Loss 0.0092\n",
      "Time taken for 1 epoch 0.18046069145202637 sec\n",
      "\n",
      "Epoch 21 Batch 0 Loss 0.0119\n",
      "Epoch 21 Loss 0.0072\n",
      "Time taken for 1 epoch 0.1219630241394043 sec\n",
      "\n",
      "Epoch 22 Batch 0 Loss 0.0109\n",
      "Epoch 22 Loss 0.0093\n",
      "Time taken for 1 epoch 0.17753291130065918 sec\n",
      "\n",
      "Epoch 23 Batch 0 Loss 0.0028\n",
      "Epoch 23 Loss 0.0092\n",
      "Time taken for 1 epoch 0.09515976905822754 sec\n",
      "\n",
      "Epoch 24 Batch 0 Loss 0.0089\n",
      "Epoch 24 Loss 0.0080\n",
      "Time taken for 1 epoch 0.18519186973571777 sec\n",
      "\n",
      "Epoch 25 Batch 0 Loss 0.0005\n",
      "Epoch 25 Loss 0.0060\n",
      "Time taken for 1 epoch 0.09606099128723145 sec\n",
      "\n",
      "Epoch 26 Batch 0 Loss 0.0119\n",
      "Epoch 26 Loss 0.0083\n",
      "Time taken for 1 epoch 0.18546295166015625 sec\n",
      "\n",
      "Epoch 27 Batch 0 Loss 0.0132\n",
      "Epoch 27 Loss 0.0092\n",
      "Time taken for 1 epoch 0.09690022468566895 sec\n",
      "\n",
      "Epoch 28 Batch 0 Loss 0.0131\n",
      "Epoch 28 Loss 0.0080\n",
      "Time taken for 1 epoch 0.20636320114135742 sec\n",
      "\n",
      "Epoch 29 Batch 0 Loss 0.0073\n",
      "Epoch 29 Loss 0.0082\n",
      "Time taken for 1 epoch 0.10663604736328125 sec\n",
      "\n",
      "Epoch 30 Batch 0 Loss 0.0005\n",
      "Epoch 30 Loss 0.0060\n",
      "Time taken for 1 epoch 0.19420909881591797 sec\n",
      "\n",
      "Epoch 31 Batch 0 Loss 0.0071\n",
      "Epoch 31 Loss 0.0071\n",
      "Time taken for 1 epoch 0.08673810958862305 sec\n",
      "\n",
      "Epoch 32 Batch 0 Loss 0.0074\n",
      "Epoch 32 Loss 0.0093\n",
      "Time taken for 1 epoch 0.16350793838500977 sec\n",
      "\n",
      "Epoch 33 Batch 0 Loss 0.0048\n",
      "Epoch 33 Loss 0.0072\n",
      "Time taken for 1 epoch 0.08996987342834473 sec\n",
      "\n",
      "Epoch 34 Batch 0 Loss 0.0114\n",
      "Epoch 34 Loss 0.0072\n",
      "Time taken for 1 epoch 0.16037607192993164 sec\n",
      "\n",
      "Epoch 35 Batch 0 Loss 0.0047\n",
      "Epoch 35 Loss 0.0092\n",
      "Time taken for 1 epoch 0.08628392219543457 sec\n",
      "\n",
      "Epoch 36 Batch 0 Loss 0.0071\n",
      "Epoch 36 Loss 0.0071\n",
      "Time taken for 1 epoch 0.15923404693603516 sec\n",
      "\n",
      "Epoch 37 Batch 0 Loss 0.0112\n",
      "Epoch 37 Loss 0.0091\n",
      "Time taken for 1 epoch 0.09110212326049805 sec\n",
      "\n",
      "Epoch 38 Batch 0 Loss 0.0070\n",
      "Epoch 38 Loss 0.0069\n",
      "Time taken for 1 epoch 0.1659247875213623 sec\n",
      "\n",
      "Epoch 39 Batch 0 Loss 0.0025\n",
      "Epoch 39 Loss 0.0060\n",
      "Time taken for 1 epoch 0.09113073348999023 sec\n",
      "\n",
      "Epoch 40 Batch 0 Loss 0.0091\n",
      "Epoch 40 Loss 0.0058\n",
      "Time taken for 1 epoch 0.16188502311706543 sec\n",
      "\n",
      "Epoch 41 Batch 0 Loss 0.0067\n",
      "Epoch 41 Loss 0.0092\n",
      "Time taken for 1 epoch 0.08693099021911621 sec\n",
      "\n",
      "Epoch 42 Batch 0 Loss 0.0048\n",
      "Epoch 42 Loss 0.0058\n",
      "Time taken for 1 epoch 0.16143584251403809 sec\n",
      "\n",
      "Epoch 43 Batch 0 Loss 0.0075\n",
      "Epoch 43 Loss 0.0070\n",
      "Time taken for 1 epoch 0.08683490753173828 sec\n",
      "\n",
      "Epoch 44 Batch 0 Loss 0.0119\n",
      "Epoch 44 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16376137733459473 sec\n",
      "\n",
      "Epoch 45 Batch 0 Loss 0.0111\n",
      "Epoch 45 Loss 0.0070\n",
      "Time taken for 1 epoch 0.08778119087219238 sec\n",
      "\n",
      "Epoch 46 Batch 0 Loss 0.0065\n",
      "Epoch 46 Loss 0.0048\n",
      "Time taken for 1 epoch 0.16399502754211426 sec\n",
      "\n",
      "Epoch 47 Batch 0 Loss 0.0091\n",
      "Epoch 47 Loss 0.0070\n",
      "Time taken for 1 epoch 0.0879521369934082 sec\n",
      "\n",
      "Epoch 48 Batch 0 Loss 0.0092\n",
      "Epoch 48 Loss 0.0083\n",
      "Time taken for 1 epoch 0.16089916229248047 sec\n",
      "\n",
      "Epoch 49 Batch 0 Loss 0.0137\n",
      "Epoch 49 Loss 0.0071\n",
      "Time taken for 1 epoch 0.08694720268249512 sec\n",
      "\n",
      "Epoch 50 Batch 0 Loss 0.0069\n",
      "Epoch 50 Loss 0.0092\n",
      "Time taken for 1 epoch 0.16052031517028809 sec\n",
      "\n",
      "Epoch 51 Batch 0 Loss 0.0096\n",
      "Epoch 51 Loss 0.0092\n",
      "Time taken for 1 epoch 0.13173484802246094 sec\n",
      "\n",
      "Epoch 52 Batch 0 Loss 0.0070\n",
      "Epoch 52 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16500401496887207 sec\n",
      "\n",
      "Epoch 53 Batch 0 Loss 0.0088\n",
      "Epoch 53 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08829283714294434 sec\n",
      "\n",
      "Epoch 54 Batch 0 Loss 0.0094\n",
      "Epoch 54 Loss 0.0082\n",
      "Time taken for 1 epoch 0.15900421142578125 sec\n",
      "\n",
      "Epoch 55 Batch 0 Loss 0.0119\n",
      "Epoch 55 Loss 0.0080\n",
      "Time taken for 1 epoch 0.0877540111541748 sec\n",
      "\n",
      "Epoch 56 Batch 0 Loss 0.0130\n",
      "Epoch 56 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16237282752990723 sec\n",
      "\n",
      "Epoch 57 Batch 0 Loss 0.0032\n",
      "Epoch 57 Loss 0.0038\n",
      "Time taken for 1 epoch 0.08554911613464355 sec\n",
      "\n",
      "Epoch 58 Batch 0 Loss 0.0103\n",
      "Epoch 58 Loss 0.0065\n",
      "Time taken for 1 epoch 0.15997004508972168 sec\n",
      "\n",
      "Epoch 59 Batch 0 Loss 0.0042\n",
      "Epoch 59 Loss 0.0050\n",
      "Time taken for 1 epoch 0.08815789222717285 sec\n",
      "\n",
      "Epoch 60 Batch 0 Loss 0.0032\n",
      "Epoch 60 Loss 0.0049\n",
      "Time taken for 1 epoch 0.16104912757873535 sec\n",
      "\n",
      "Epoch 61 Batch 0 Loss 0.0073\n",
      "Epoch 61 Loss 0.0083\n",
      "Time taken for 1 epoch 0.10924386978149414 sec\n",
      "\n",
      "Epoch 62 Batch 0 Loss 0.0066\n",
      "Epoch 62 Loss 0.0092\n",
      "Time taken for 1 epoch 0.16307997703552246 sec\n",
      "\n",
      "Epoch 63 Batch 0 Loss 0.0119\n",
      "Epoch 63 Loss 0.0092\n",
      "Time taken for 1 epoch 0.08689713478088379 sec\n",
      "\n",
      "Epoch 64 Batch 0 Loss 0.0137\n",
      "Epoch 64 Loss 0.0071\n",
      "Time taken for 1 epoch 0.15908598899841309 sec\n",
      "\n",
      "Epoch 65 Batch 0 Loss 0.0070\n",
      "Epoch 65 Loss 0.0057\n",
      "Time taken for 1 epoch 0.08867716789245605 sec\n",
      "\n",
      "Epoch 66 Batch 0 Loss 0.0068\n",
      "Epoch 66 Loss 0.0078\n",
      "Time taken for 1 epoch 0.16304397583007812 sec\n",
      "\n",
      "Epoch 67 Batch 0 Loss 0.0107\n",
      "Epoch 67 Loss 0.0080\n",
      "Time taken for 1 epoch 0.0884101390838623 sec\n",
      "\n",
      "Epoch 68 Batch 0 Loss 0.0103\n",
      "Epoch 68 Loss 0.0078\n",
      "Time taken for 1 epoch 0.1614220142364502 sec\n",
      "\n",
      "Epoch 69 Batch 0 Loss 0.0045\n",
      "Epoch 69 Loss 0.0056\n",
      "Time taken for 1 epoch 0.08578085899353027 sec\n",
      "\n",
      "Epoch 70 Batch 0 Loss 0.0063\n",
      "Epoch 70 Loss 0.0034\n",
      "Time taken for 1 epoch 0.16403579711914062 sec\n",
      "\n",
      "Epoch 71 Batch 0 Loss 0.0086\n",
      "Epoch 71 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08475899696350098 sec\n",
      "\n",
      "Epoch 72 Batch 0 Loss 0.0119\n",
      "Epoch 72 Loss 0.0093\n",
      "Time taken for 1 epoch 0.16321325302124023 sec\n",
      "\n",
      "Epoch 73 Batch 0 Loss 0.0147\n",
      "Epoch 73 Loss 0.0076\n",
      "Time taken for 1 epoch 0.08436012268066406 sec\n",
      "\n",
      "Epoch 74 Batch 0 Loss 0.0066\n",
      "Epoch 74 Loss 0.0067\n",
      "Time taken for 1 epoch 0.1621718406677246 sec\n",
      "\n",
      "Epoch 75 Batch 0 Loss 0.0051\n",
      "Epoch 75 Loss 0.0084\n",
      "Time taken for 1 epoch 0.08952212333679199 sec\n",
      "\n",
      "Epoch 76 Batch 0 Loss 0.0062\n",
      "Epoch 76 Loss 0.0061\n",
      "Time taken for 1 epoch 0.16334319114685059 sec\n",
      "\n",
      "Epoch 77 Batch 0 Loss 0.0028\n",
      "Epoch 77 Loss 0.0093\n",
      "Time taken for 1 epoch 0.0905451774597168 sec\n",
      "\n",
      "Epoch 78 Batch 0 Loss 0.0048\n",
      "Epoch 78 Loss 0.0072\n",
      "Time taken for 1 epoch 0.16237998008728027 sec\n",
      "\n",
      "Epoch 79 Batch 0 Loss 0.0066\n",
      "Epoch 79 Loss 0.0077\n",
      "Time taken for 1 epoch 0.0862739086151123 sec\n",
      "\n",
      "Epoch 80 Batch 0 Loss 0.0065\n",
      "Epoch 80 Loss 0.0065\n",
      "Time taken for 1 epoch 0.1623530387878418 sec\n",
      "\n",
      "Epoch 81 Batch 0 Loss 0.0092\n",
      "Epoch 81 Loss 0.0077\n",
      "Time taken for 1 epoch 0.0923299789428711 sec\n",
      "\n",
      "Epoch 82 Batch 0 Loss 0.0120\n",
      "Epoch 82 Loss 0.0093\n",
      "Time taken for 1 epoch 0.1629481315612793 sec\n",
      "\n",
      "Epoch 83 Batch 0 Loss 0.0102\n",
      "Epoch 83 Loss 0.0053\n",
      "Time taken for 1 epoch 0.08572793006896973 sec\n",
      "\n",
      "Epoch 84 Batch 0 Loss 0.0135\n",
      "Epoch 84 Loss 0.0085\n",
      "Time taken for 1 epoch 0.1582958698272705 sec\n",
      "\n",
      "Epoch 85 Batch 0 Loss 0.0106\n",
      "Epoch 85 Loss 0.0077\n",
      "Time taken for 1 epoch 0.08715677261352539 sec\n",
      "\n",
      "Epoch 86 Batch 0 Loss 0.0080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86 Loss 0.0082\n",
      "Time taken for 1 epoch 0.15804219245910645 sec\n",
      "\n",
      "Epoch 87 Batch 0 Loss 0.0063\n",
      "Epoch 87 Loss 0.0092\n",
      "Time taken for 1 epoch 0.0880889892578125 sec\n",
      "\n",
      "Epoch 88 Batch 0 Loss 0.0071\n",
      "Epoch 88 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16056180000305176 sec\n",
      "\n",
      "Epoch 89 Batch 0 Loss 0.0028\n",
      "Epoch 89 Loss 0.0067\n",
      "Time taken for 1 epoch 0.08514595031738281 sec\n",
      "\n",
      "Epoch 90 Batch 0 Loss 0.0089\n",
      "Epoch 90 Loss 0.0059\n",
      "Time taken for 1 epoch 0.15961503982543945 sec\n",
      "\n",
      "Epoch 91 Batch 0 Loss 0.0047\n",
      "Epoch 91 Loss 0.0071\n",
      "Time taken for 1 epoch 0.08791708946228027 sec\n",
      "\n",
      "Epoch 92 Batch 0 Loss 0.0044\n",
      "Epoch 92 Loss 0.0048\n",
      "Time taken for 1 epoch 0.1611461639404297 sec\n",
      "\n",
      "Epoch 93 Batch 0 Loss 0.0097\n",
      "Epoch 93 Loss 0.0070\n",
      "Time taken for 1 epoch 0.08649778366088867 sec\n",
      "\n",
      "Epoch 94 Batch 0 Loss 0.0097\n",
      "Epoch 94 Loss 0.0060\n",
      "Time taken for 1 epoch 0.1599578857421875 sec\n",
      "\n",
      "Epoch 95 Batch 0 Loss 0.0024\n",
      "Epoch 95 Loss 0.0070\n",
      "Time taken for 1 epoch 0.08831524848937988 sec\n",
      "\n",
      "Epoch 96 Batch 0 Loss 0.0024\n",
      "Epoch 96 Loss 0.0081\n",
      "Time taken for 1 epoch 0.1570141315460205 sec\n",
      "\n",
      "Epoch 97 Batch 0 Loss 0.0091\n",
      "Epoch 97 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08683896064758301 sec\n",
      "\n",
      "Epoch 98 Batch 0 Loss 0.0049\n",
      "Epoch 98 Loss 0.0058\n",
      "Time taken for 1 epoch 0.16263890266418457 sec\n",
      "\n",
      "Epoch 99 Batch 0 Loss 0.0134\n",
      "Epoch 99 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08232975006103516 sec\n",
      "\n",
      "Epoch 100 Batch 0 Loss 0.0070\n",
      "Epoch 100 Loss 0.0082\n",
      "Time taken for 1 epoch 0.15967583656311035 sec\n",
      "\n",
      "Epoch 101 Batch 0 Loss 0.0092\n",
      "Epoch 101 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08699297904968262 sec\n",
      "\n",
      "Epoch 102 Batch 0 Loss 0.0065\n",
      "Epoch 102 Loss 0.0091\n",
      "Time taken for 1 epoch 0.1622636318206787 sec\n",
      "\n",
      "Epoch 103 Batch 0 Loss 0.0107\n",
      "Epoch 103 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08912897109985352 sec\n",
      "\n",
      "Epoch 104 Batch 0 Loss 0.0115\n",
      "Epoch 104 Loss 0.0080\n",
      "Time taken for 1 epoch 0.159959077835083 sec\n",
      "\n",
      "Epoch 105 Batch 0 Loss 0.0117\n",
      "Epoch 105 Loss 0.0083\n",
      "Time taken for 1 epoch 0.08667111396789551 sec\n",
      "\n",
      "Epoch 106 Batch 0 Loss 0.0073\n",
      "Epoch 106 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16043996810913086 sec\n",
      "\n",
      "Epoch 107 Batch 0 Loss 0.0113\n",
      "Epoch 107 Loss 0.0091\n",
      "Time taken for 1 epoch 0.08813786506652832 sec\n",
      "\n",
      "Epoch 108 Batch 0 Loss 0.0047\n",
      "Epoch 108 Loss 0.0069\n",
      "Time taken for 1 epoch 0.16134405136108398 sec\n",
      "\n",
      "Epoch 109 Batch 0 Loss 0.0110\n",
      "Epoch 109 Loss 0.0066\n",
      "Time taken for 1 epoch 0.08667898178100586 sec\n",
      "\n",
      "Epoch 110 Batch 0 Loss 0.0028\n",
      "Epoch 110 Loss 0.0082\n",
      "Time taken for 1 epoch 0.15851783752441406 sec\n",
      "\n",
      "Epoch 111 Batch 0 Loss 0.0041\n",
      "Epoch 111 Loss 0.0092\n",
      "Time taken for 1 epoch 0.0881490707397461 sec\n",
      "\n",
      "Epoch 112 Batch 0 Loss 0.0160\n",
      "Epoch 112 Loss 0.0091\n",
      "Time taken for 1 epoch 0.16136384010314941 sec\n",
      "\n",
      "Epoch 113 Batch 0 Loss 0.0068\n",
      "Epoch 113 Loss 0.0070\n",
      "Time taken for 1 epoch 0.08824896812438965 sec\n",
      "\n",
      "Epoch 114 Batch 0 Loss 0.0065\n",
      "Epoch 114 Loss 0.0056\n",
      "Time taken for 1 epoch 0.15897917747497559 sec\n",
      "\n",
      "Epoch 115 Batch 0 Loss 0.0065\n",
      "Epoch 115 Loss 0.0058\n",
      "Time taken for 1 epoch 0.08793807029724121 sec\n",
      "\n",
      "Epoch 116 Batch 0 Loss 0.0045\n",
      "Epoch 116 Loss 0.0046\n",
      "Time taken for 1 epoch 0.15917491912841797 sec\n",
      "\n",
      "Epoch 117 Batch 0 Loss 0.0068\n",
      "Epoch 117 Loss 0.0082\n",
      "Time taken for 1 epoch 0.09102511405944824 sec\n",
      "\n",
      "Epoch 118 Batch 0 Loss 0.0096\n",
      "Epoch 118 Loss 0.0092\n",
      "Time taken for 1 epoch 0.16088199615478516 sec\n",
      "\n",
      "Epoch 119 Batch 0 Loss 0.0062\n",
      "Epoch 119 Loss 0.0059\n",
      "Time taken for 1 epoch 0.08657002449035645 sec\n",
      "\n",
      "Epoch 120 Batch 0 Loss 0.0162\n",
      "Epoch 120 Loss 0.0091\n",
      "Time taken for 1 epoch 0.15843820571899414 sec\n",
      "\n",
      "Epoch 121 Batch 0 Loss 0.0084\n",
      "Epoch 121 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08743715286254883 sec\n",
      "\n",
      "Epoch 122 Batch 0 Loss 0.0090\n",
      "Epoch 122 Loss 0.0072\n",
      "Time taken for 1 epoch 0.16644501686096191 sec\n",
      "\n",
      "Epoch 123 Batch 0 Loss 0.0113\n",
      "Epoch 123 Loss 0.0091\n",
      "Time taken for 1 epoch 0.0921938419342041 sec\n",
      "\n",
      "Epoch 124 Batch 0 Loss 0.0090\n",
      "Epoch 124 Loss 0.0057\n",
      "Time taken for 1 epoch 0.16031599044799805 sec\n",
      "\n",
      "Epoch 125 Batch 0 Loss 0.0068\n",
      "Epoch 125 Loss 0.0079\n",
      "Time taken for 1 epoch 0.0878000259399414 sec\n",
      "\n",
      "Epoch 126 Batch 0 Loss 0.0091\n",
      "Epoch 126 Loss 0.0070\n",
      "Time taken for 1 epoch 0.1595320701599121 sec\n",
      "\n",
      "Epoch 127 Batch 0 Loss 0.0110\n",
      "Epoch 127 Loss 0.0082\n",
      "Time taken for 1 epoch 0.08850502967834473 sec\n",
      "\n",
      "Epoch 128 Batch 0 Loss 0.0068\n",
      "Epoch 128 Loss 0.0079\n",
      "Time taken for 1 epoch 0.1595020294189453 sec\n",
      "\n",
      "Epoch 129 Batch 0 Loss 0.0072\n",
      "Epoch 129 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08927774429321289 sec\n",
      "\n",
      "Epoch 130 Batch 0 Loss 0.0063\n",
      "Epoch 130 Loss 0.0091\n",
      "Time taken for 1 epoch 0.15927886962890625 sec\n",
      "\n",
      "Epoch 131 Batch 0 Loss 0.0029\n",
      "Epoch 131 Loss 0.0082\n",
      "Time taken for 1 epoch 0.0883939266204834 sec\n",
      "\n",
      "Epoch 132 Batch 0 Loss 0.0066\n",
      "Epoch 132 Loss 0.0081\n",
      "Time taken for 1 epoch 0.15993523597717285 sec\n",
      "\n",
      "Epoch 133 Batch 0 Loss 0.0047\n",
      "Epoch 133 Loss 0.0071\n",
      "Time taken for 1 epoch 0.08926200866699219 sec\n",
      "\n",
      "Epoch 134 Batch 0 Loss 0.0091\n",
      "Epoch 134 Loss 0.0079\n",
      "Time taken for 1 epoch 0.15993809700012207 sec\n",
      "\n",
      "Epoch 135 Batch 0 Loss 0.0069\n",
      "Epoch 135 Loss 0.0047\n",
      "Time taken for 1 epoch 0.08740401268005371 sec\n",
      "\n",
      "Epoch 136 Batch 0 Loss 0.0047\n",
      "Epoch 136 Loss 0.0047\n",
      "Time taken for 1 epoch 0.15891504287719727 sec\n",
      "\n",
      "Epoch 137 Batch 0 Loss 0.0045\n",
      "Epoch 137 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08843016624450684 sec\n",
      "\n",
      "Epoch 138 Batch 0 Loss 0.0022\n",
      "Epoch 138 Loss 0.0082\n",
      "Time taken for 1 epoch 0.1608288288116455 sec\n",
      "\n",
      "Epoch 139 Batch 0 Loss 0.0051\n",
      "Epoch 139 Loss 0.0061\n",
      "Time taken for 1 epoch 0.08577489852905273 sec\n",
      "\n",
      "Epoch 140 Batch 0 Loss 0.0064\n",
      "Epoch 140 Loss 0.0065\n",
      "Time taken for 1 epoch 0.16222906112670898 sec\n",
      "\n",
      "Epoch 141 Batch 0 Loss 0.0074\n",
      "Epoch 141 Loss 0.0092\n",
      "Time taken for 1 epoch 0.08837389945983887 sec\n",
      "\n",
      "Epoch 142 Batch 0 Loss 0.0004\n",
      "Epoch 142 Loss 0.0044\n",
      "Time taken for 1 epoch 0.16018199920654297 sec\n",
      "\n",
      "Epoch 143 Batch 0 Loss 0.0069\n",
      "Epoch 143 Loss 0.0070\n",
      "Time taken for 1 epoch 0.08810782432556152 sec\n",
      "\n",
      "Epoch 144 Batch 0 Loss 0.0066\n",
      "Epoch 144 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16169023513793945 sec\n",
      "\n",
      "Epoch 145 Batch 0 Loss 0.0086\n",
      "Epoch 145 Loss 0.0055\n",
      "Time taken for 1 epoch 0.08756399154663086 sec\n",
      "\n",
      "Epoch 146 Batch 0 Loss 0.0056\n",
      "Epoch 146 Loss 0.0060\n",
      "Time taken for 1 epoch 0.15841913223266602 sec\n",
      "\n",
      "Epoch 147 Batch 0 Loss 0.0091\n",
      "Epoch 147 Loss 0.0092\n",
      "Time taken for 1 epoch 0.0998380184173584 sec\n",
      "\n",
      "Epoch 148 Batch 0 Loss 0.0070\n",
      "Epoch 148 Loss 0.0092\n",
      "Time taken for 1 epoch 0.16381382942199707 sec\n",
      "\n",
      "Epoch 149 Batch 0 Loss 0.0063\n",
      "Epoch 149 Loss 0.0062\n",
      "Time taken for 1 epoch 0.08581018447875977 sec\n",
      "\n",
      "Epoch 150 Batch 0 Loss 0.0083\n",
      "Epoch 150 Loss 0.0079\n",
      "Time taken for 1 epoch 0.16131114959716797 sec\n",
      "\n",
      "Epoch 151 Batch 0 Loss 0.0048\n",
      "Epoch 151 Loss 0.0082\n",
      "Time taken for 1 epoch 0.08632993698120117 sec\n",
      "\n",
      "Epoch 152 Batch 0 Loss 0.0073\n",
      "Epoch 152 Loss 0.0092\n",
      "Time taken for 1 epoch 0.16258907318115234 sec\n",
      "\n",
      "Epoch 153 Batch 0 Loss 0.0070\n",
      "Epoch 153 Loss 0.0067\n",
      "Time taken for 1 epoch 0.08565020561218262 sec\n",
      "\n",
      "Epoch 154 Batch 0 Loss 0.0052\n",
      "Epoch 154 Loss 0.0082\n",
      "Time taken for 1 epoch 0.1599750518798828 sec\n",
      "\n",
      "Epoch 155 Batch 0 Loss 0.0023\n",
      "Epoch 155 Loss 0.0082\n",
      "Time taken for 1 epoch 0.08602190017700195 sec\n",
      "\n",
      "Epoch 156 Batch 0 Loss 0.0099\n",
      "Epoch 156 Loss 0.0073\n",
      "Time taken for 1 epoch 0.1606590747833252 sec\n",
      "\n",
      "Epoch 157 Batch 0 Loss 0.0110\n",
      "Epoch 157 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08611583709716797 sec\n",
      "\n",
      "Epoch 158 Batch 0 Loss 0.0048\n",
      "Epoch 158 Loss 0.0057\n",
      "Time taken for 1 epoch 0.15884089469909668 sec\n",
      "\n",
      "Epoch 159 Batch 0 Loss 0.0090\n",
      "Epoch 159 Loss 0.0056\n",
      "Time taken for 1 epoch 0.08800578117370605 sec\n",
      "\n",
      "Epoch 160 Batch 0 Loss 0.0112\n",
      "Epoch 160 Loss 0.0082\n",
      "Time taken for 1 epoch 0.16080904006958008 sec\n",
      "\n",
      "Epoch 161 Batch 0 Loss 0.0134\n",
      "Epoch 161 Loss 0.0091\n",
      "Time taken for 1 epoch 0.08789587020874023 sec\n",
      "\n",
      "Epoch 162 Batch 0 Loss 0.0071\n",
      "Epoch 162 Loss 0.0081\n",
      "Time taken for 1 epoch 0.15893197059631348 sec\n",
      "\n",
      "Epoch 163 Batch 0 Loss 0.0027\n",
      "Epoch 163 Loss 0.0069\n",
      "Time taken for 1 epoch 0.0908510684967041 sec\n",
      "\n",
      "Epoch 164 Batch 0 Loss 0.0068\n",
      "Epoch 164 Loss 0.0070\n",
      "Time taken for 1 epoch 0.16716790199279785 sec\n",
      "\n",
      "Epoch 165 Batch 0 Loss 0.0046\n",
      "Epoch 165 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08801007270812988 sec\n",
      "\n",
      "Epoch 166 Batch 0 Loss 0.0135\n",
      "Epoch 166 Loss 0.0091\n",
      "Time taken for 1 epoch 0.1578381061553955 sec\n",
      "\n",
      "Epoch 167 Batch 0 Loss 0.0091\n",
      "Epoch 167 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08599710464477539 sec\n",
      "\n",
      "Epoch 168 Batch 0 Loss 0.0070\n",
      "Epoch 168 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16138720512390137 sec\n",
      "\n",
      "Epoch 169 Batch 0 Loss 0.0047\n",
      "Epoch 169 Loss 0.0081\n",
      "Time taken for 1 epoch 0.08561491966247559 sec\n",
      "\n",
      "Epoch 170 Batch 0 Loss 0.0047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 170 Loss 0.0080\n",
      "Time taken for 1 epoch 0.15845894813537598 sec\n",
      "\n",
      "Epoch 171 Batch 0 Loss 0.0025\n",
      "Epoch 171 Loss 0.0037\n",
      "Time taken for 1 epoch 0.08760190010070801 sec\n",
      "\n",
      "Epoch 172 Batch 0 Loss 0.0026\n",
      "Epoch 172 Loss 0.0058\n",
      "Time taken for 1 epoch 0.15862703323364258 sec\n",
      "\n",
      "Epoch 173 Batch 0 Loss 0.0046\n",
      "Epoch 173 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08713173866271973 sec\n",
      "\n",
      "Epoch 174 Batch 0 Loss 0.0046\n",
      "Epoch 174 Loss 0.0060\n",
      "Time taken for 1 epoch 0.16225624084472656 sec\n",
      "\n",
      "Epoch 175 Batch 0 Loss 0.0111\n",
      "Epoch 175 Loss 0.0091\n",
      "Time taken for 1 epoch 0.08931708335876465 sec\n",
      "\n",
      "Epoch 176 Batch 0 Loss 0.0023\n",
      "Epoch 176 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16410112380981445 sec\n",
      "\n",
      "Epoch 177 Batch 0 Loss 0.0064\n",
      "Epoch 177 Loss 0.0079\n",
      "Time taken for 1 epoch 0.0853111743927002 sec\n",
      "\n",
      "Epoch 178 Batch 0 Loss 0.0118\n",
      "Epoch 178 Loss 0.0081\n",
      "Time taken for 1 epoch 0.1591031551361084 sec\n",
      "\n",
      "Epoch 179 Batch 0 Loss 0.0090\n",
      "Epoch 179 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08842992782592773 sec\n",
      "\n",
      "Epoch 180 Batch 0 Loss 0.0110\n",
      "Epoch 180 Loss 0.0078\n",
      "Time taken for 1 epoch 0.16047310829162598 sec\n",
      "\n",
      "Epoch 181 Batch 0 Loss 0.0090\n",
      "Epoch 181 Loss 0.0070\n",
      "Time taken for 1 epoch 0.08749794960021973 sec\n",
      "\n",
      "Epoch 182 Batch 0 Loss 0.0027\n",
      "Epoch 182 Loss 0.0069\n",
      "Time taken for 1 epoch 0.160567045211792 sec\n",
      "\n",
      "Epoch 183 Batch 0 Loss 0.0114\n",
      "Epoch 183 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08758711814880371 sec\n",
      "\n",
      "Epoch 184 Batch 0 Loss 0.0051\n",
      "Epoch 184 Loss 0.0059\n",
      "Time taken for 1 epoch 0.15987896919250488 sec\n",
      "\n",
      "Epoch 185 Batch 0 Loss 0.0066\n",
      "Epoch 185 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08807802200317383 sec\n",
      "\n",
      "Epoch 186 Batch 0 Loss 0.0069\n",
      "Epoch 186 Loss 0.0091\n",
      "Time taken for 1 epoch 0.16074395179748535 sec\n",
      "\n",
      "Epoch 187 Batch 0 Loss 0.0113\n",
      "Epoch 187 Loss 0.0091\n",
      "Time taken for 1 epoch 0.0855717658996582 sec\n",
      "\n",
      "Epoch 188 Batch 0 Loss 0.0118\n",
      "Epoch 188 Loss 0.0071\n",
      "Time taken for 1 epoch 0.1695549488067627 sec\n",
      "\n",
      "Epoch 189 Batch 0 Loss 0.0083\n",
      "Epoch 189 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08856487274169922 sec\n",
      "\n",
      "Epoch 190 Batch 0 Loss 0.0108\n",
      "Epoch 190 Loss 0.0078\n",
      "Time taken for 1 epoch 0.16044902801513672 sec\n",
      "\n",
      "Epoch 191 Batch 0 Loss 0.0021\n",
      "Epoch 191 Loss 0.0039\n",
      "Time taken for 1 epoch 0.0886080265045166 sec\n",
      "\n",
      "Epoch 192 Batch 0 Loss 0.0135\n",
      "Epoch 192 Loss 0.0091\n",
      "Time taken for 1 epoch 0.1608409881591797 sec\n",
      "\n",
      "Epoch 193 Batch 0 Loss 0.0047\n",
      "Epoch 193 Loss 0.0058\n",
      "Time taken for 1 epoch 0.08767414093017578 sec\n",
      "\n",
      "Epoch 194 Batch 0 Loss 0.0029\n",
      "Epoch 194 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16087698936462402 sec\n",
      "\n",
      "Epoch 195 Batch 0 Loss 0.0119\n",
      "Epoch 195 Loss 0.0081\n",
      "Time taken for 1 epoch 0.08712601661682129 sec\n",
      "\n",
      "Epoch 196 Batch 0 Loss 0.0106\n",
      "Epoch 196 Loss 0.0065\n",
      "Time taken for 1 epoch 0.15955877304077148 sec\n",
      "\n",
      "Epoch 197 Batch 0 Loss 0.0040\n",
      "Epoch 197 Loss 0.0054\n",
      "Time taken for 1 epoch 0.08780527114868164 sec\n",
      "\n",
      "Epoch 198 Batch 0 Loss 0.0088\n",
      "Epoch 198 Loss 0.0045\n",
      "Time taken for 1 epoch 0.15848684310913086 sec\n",
      "\n",
      "Epoch 199 Batch 0 Loss 0.0037\n",
      "Epoch 199 Loss 0.0070\n",
      "Time taken for 1 epoch 0.08912420272827148 sec\n",
      "\n",
      "Epoch 200 Batch 0 Loss 0.0018\n",
      "Epoch 200 Loss 0.0079\n",
      "Time taken for 1 epoch 0.16164016723632812 sec\n",
      "\n",
      "Epoch 201 Batch 0 Loss 0.0067\n",
      "Epoch 201 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08848094940185547 sec\n",
      "\n",
      "Epoch 202 Batch 0 Loss 0.0105\n",
      "Epoch 202 Loss 0.0069\n",
      "Time taken for 1 epoch 0.16148018836975098 sec\n",
      "\n",
      "Epoch 203 Batch 0 Loss 0.0065\n",
      "Epoch 203 Loss 0.0083\n",
      "Time taken for 1 epoch 0.08626317977905273 sec\n",
      "\n",
      "Epoch 204 Batch 0 Loss 0.0038\n",
      "Epoch 204 Loss 0.0054\n",
      "Time taken for 1 epoch 0.1632699966430664 sec\n",
      "\n",
      "Epoch 205 Batch 0 Loss 0.0063\n",
      "Epoch 205 Loss 0.0077\n",
      "Time taken for 1 epoch 0.08991885185241699 sec\n",
      "\n",
      "Epoch 206 Batch 0 Loss 0.0056\n",
      "Epoch 206 Loss 0.0070\n",
      "Time taken for 1 epoch 0.1621088981628418 sec\n",
      "\n",
      "Epoch 207 Batch 0 Loss 0.0089\n",
      "Epoch 207 Loss 0.0092\n",
      "Time taken for 1 epoch 0.08665776252746582 sec\n",
      "\n",
      "Epoch 208 Batch 0 Loss 0.0079\n",
      "Epoch 208 Loss 0.0076\n",
      "Time taken for 1 epoch 0.3680896759033203 sec\n",
      "\n",
      "Epoch 209 Batch 0 Loss 0.0018\n",
      "Epoch 209 Loss 0.0059\n",
      "Time taken for 1 epoch 0.08624482154846191 sec\n",
      "\n",
      "Epoch 210 Batch 0 Loss 0.0101\n",
      "Epoch 210 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16823816299438477 sec\n",
      "\n",
      "Epoch 211 Batch 0 Loss 0.0062\n",
      "Epoch 211 Loss 0.0054\n",
      "Time taken for 1 epoch 0.08725476264953613 sec\n",
      "\n",
      "Epoch 212 Batch 0 Loss 0.0072\n",
      "Epoch 212 Loss 0.0070\n",
      "Time taken for 1 epoch 0.16202998161315918 sec\n",
      "\n",
      "Epoch 213 Batch 0 Loss 0.0061\n",
      "Epoch 213 Loss 0.0053\n",
      "Time taken for 1 epoch 0.08650612831115723 sec\n",
      "\n",
      "Epoch 214 Batch 0 Loss 0.0101\n",
      "Epoch 214 Loss 0.0092\n",
      "Time taken for 1 epoch 0.16077518463134766 sec\n",
      "\n",
      "Epoch 215 Batch 0 Loss 0.0084\n",
      "Epoch 215 Loss 0.0092\n",
      "Time taken for 1 epoch 0.08572220802307129 sec\n",
      "\n",
      "Epoch 216 Batch 0 Loss 0.0098\n",
      "Epoch 216 Loss 0.0066\n",
      "Time taken for 1 epoch 0.15816283226013184 sec\n",
      "\n",
      "Epoch 217 Batch 0 Loss 0.0019\n",
      "Epoch 217 Loss 0.0072\n",
      "Time taken for 1 epoch 0.08892107009887695 sec\n",
      "\n",
      "Epoch 218 Batch 0 Loss 0.0084\n",
      "Epoch 218 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16079092025756836 sec\n",
      "\n",
      "Epoch 219 Batch 0 Loss 0.0093\n",
      "Epoch 219 Loss 0.0081\n",
      "Time taken for 1 epoch 0.08837413787841797 sec\n",
      "\n",
      "Epoch 220 Batch 0 Loss 0.0048\n",
      "Epoch 220 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16022992134094238 sec\n",
      "\n",
      "Epoch 221 Batch 0 Loss 0.0068\n",
      "Epoch 221 Loss 0.0057\n",
      "Time taken for 1 epoch 0.0883948802947998 sec\n",
      "\n",
      "Epoch 222 Batch 0 Loss 0.0051\n",
      "Epoch 222 Loss 0.0047\n",
      "Time taken for 1 epoch 0.16180896759033203 sec\n",
      "\n",
      "Epoch 223 Batch 0 Loss 0.0110\n",
      "Epoch 223 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08629417419433594 sec\n",
      "\n",
      "Epoch 224 Batch 0 Loss 0.0003\n",
      "Epoch 224 Loss 0.0059\n",
      "Time taken for 1 epoch 0.1627352237701416 sec\n",
      "\n",
      "Epoch 225 Batch 0 Loss 0.0091\n",
      "Epoch 225 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08665323257446289 sec\n",
      "\n",
      "Epoch 226 Batch 0 Loss 0.0079\n",
      "Epoch 226 Loss 0.0053\n",
      "Time taken for 1 epoch 0.16016101837158203 sec\n",
      "\n",
      "Epoch 227 Batch 0 Loss 0.0091\n",
      "Epoch 227 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08718514442443848 sec\n",
      "\n",
      "Epoch 228 Batch 0 Loss 0.0131\n",
      "Epoch 228 Loss 0.0092\n",
      "Time taken for 1 epoch 0.15971612930297852 sec\n",
      "\n",
      "Epoch 229 Batch 0 Loss 0.0057\n",
      "Epoch 229 Loss 0.0081\n",
      "Time taken for 1 epoch 0.08777809143066406 sec\n",
      "\n",
      "Epoch 230 Batch 0 Loss 0.0084\n",
      "Epoch 230 Loss 0.0078\n",
      "Time taken for 1 epoch 0.16027212142944336 sec\n",
      "\n",
      "Epoch 231 Batch 0 Loss 0.0056\n",
      "Epoch 231 Loss 0.0084\n",
      "Time taken for 1 epoch 0.0886080265045166 sec\n",
      "\n",
      "Epoch 232 Batch 0 Loss 0.0090\n",
      "Epoch 232 Loss 0.0092\n",
      "Time taken for 1 epoch 0.1618208885192871 sec\n",
      "\n",
      "Epoch 233 Batch 0 Loss 0.0066\n",
      "Epoch 233 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08611774444580078 sec\n",
      "\n",
      "Epoch 234 Batch 0 Loss 0.0090\n",
      "Epoch 234 Loss 0.0091\n",
      "Time taken for 1 epoch 0.15840625762939453 sec\n",
      "\n",
      "Epoch 235 Batch 0 Loss 0.0108\n",
      "Epoch 235 Loss 0.0091\n",
      "Time taken for 1 epoch 0.08805203437805176 sec\n",
      "\n",
      "Epoch 236 Batch 0 Loss 0.0029\n",
      "Epoch 236 Loss 0.0091\n",
      "Time taken for 1 epoch 0.16244292259216309 sec\n",
      "\n",
      "Epoch 237 Batch 0 Loss 0.0115\n",
      "Epoch 237 Loss 0.0091\n",
      "Time taken for 1 epoch 0.08715415000915527 sec\n",
      "\n",
      "Epoch 238 Batch 0 Loss 0.0041\n",
      "Epoch 238 Loss 0.0078\n",
      "Time taken for 1 epoch 0.15843987464904785 sec\n",
      "\n",
      "Epoch 239 Batch 0 Loss 0.0089\n",
      "Epoch 239 Loss 0.0065\n",
      "Time taken for 1 epoch 0.08471989631652832 sec\n",
      "\n",
      "Epoch 240 Batch 0 Loss 0.0050\n",
      "Epoch 240 Loss 0.0070\n",
      "Time taken for 1 epoch 0.162489652633667 sec\n",
      "\n",
      "Epoch 241 Batch 0 Loss 0.0064\n",
      "Epoch 241 Loss 0.0045\n",
      "Time taken for 1 epoch 0.08742499351501465 sec\n",
      "\n",
      "Epoch 242 Batch 0 Loss 0.0070\n",
      "Epoch 242 Loss 0.0076\n",
      "Time taken for 1 epoch 0.16340184211730957 sec\n",
      "\n",
      "Epoch 243 Batch 0 Loss 0.0067\n",
      "Epoch 243 Loss 0.0060\n",
      "Time taken for 1 epoch 0.08927798271179199 sec\n",
      "\n",
      "Epoch 244 Batch 0 Loss 0.0027\n",
      "Epoch 244 Loss 0.0032\n",
      "Time taken for 1 epoch 0.16238093376159668 sec\n",
      "\n",
      "Epoch 245 Batch 0 Loss 0.0085\n",
      "Epoch 245 Loss 0.0064\n",
      "Time taken for 1 epoch 0.08630895614624023 sec\n",
      "\n",
      "Epoch 246 Batch 0 Loss 0.0065\n",
      "Epoch 246 Loss 0.0082\n",
      "Time taken for 1 epoch 0.1654191017150879 sec\n",
      "\n",
      "Epoch 247 Batch 0 Loss 0.0095\n",
      "Epoch 247 Loss 0.0049\n",
      "Time taken for 1 epoch 0.08844304084777832 sec\n",
      "\n",
      "Epoch 248 Batch 0 Loss 0.0093\n",
      "Epoch 248 Loss 0.0091\n",
      "Time taken for 1 epoch 0.16609692573547363 sec\n",
      "\n",
      "Epoch 249 Batch 0 Loss 0.0079\n",
      "Epoch 249 Loss 0.0082\n",
      "Time taken for 1 epoch 0.09287714958190918 sec\n",
      "\n",
      "Epoch 250 Batch 0 Loss 0.0023\n",
      "Epoch 250 Loss 0.0054\n",
      "Time taken for 1 epoch 0.16904282569885254 sec\n",
      "\n",
      "Epoch 251 Batch 0 Loss 0.0066\n",
      "Epoch 251 Loss 0.0045\n",
      "Time taken for 1 epoch 0.08792281150817871 sec\n",
      "\n",
      "Epoch 252 Batch 0 Loss 0.0071\n",
      "Epoch 252 Loss 0.0070\n",
      "Time taken for 1 epoch 0.16817903518676758 sec\n",
      "\n",
      "Epoch 253 Batch 0 Loss 0.0136\n",
      "Epoch 253 Loss 0.0091\n",
      "Time taken for 1 epoch 0.08777284622192383 sec\n",
      "\n",
      "Epoch 254 Batch 0 Loss 0.0068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 254 Loss 0.0053\n",
      "Time taken for 1 epoch 0.1746528148651123 sec\n",
      "\n",
      "Epoch 255 Batch 0 Loss 0.0058\n",
      "Epoch 255 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08748698234558105 sec\n",
      "\n",
      "Epoch 256 Batch 0 Loss 0.0003\n",
      "Epoch 256 Loss 0.0037\n",
      "Time taken for 1 epoch 0.16765499114990234 sec\n",
      "\n",
      "Epoch 257 Batch 0 Loss 0.0140\n",
      "Epoch 257 Loss 0.0081\n",
      "Time taken for 1 epoch 0.10105180740356445 sec\n",
      "\n",
      "Epoch 258 Batch 0 Loss 0.0142\n",
      "Epoch 258 Loss 0.0091\n",
      "Time taken for 1 epoch 0.16910314559936523 sec\n",
      "\n",
      "Epoch 259 Batch 0 Loss 0.0081\n",
      "Epoch 259 Loss 0.0056\n",
      "Time taken for 1 epoch 0.08582806587219238 sec\n",
      "\n",
      "Epoch 260 Batch 0 Loss 0.0072\n",
      "Epoch 260 Loss 0.0083\n",
      "Time taken for 1 epoch 0.16904997825622559 sec\n",
      "\n",
      "Epoch 261 Batch 0 Loss 0.0053\n",
      "Epoch 261 Loss 0.0046\n",
      "Time taken for 1 epoch 0.09167194366455078 sec\n",
      "\n",
      "Epoch 262 Batch 0 Loss 0.0042\n",
      "Epoch 262 Loss 0.0091\n",
      "Time taken for 1 epoch 0.17919301986694336 sec\n",
      "\n",
      "Epoch 263 Batch 0 Loss 0.0076\n",
      "Epoch 263 Loss 0.0081\n",
      "Time taken for 1 epoch 0.09144306182861328 sec\n",
      "\n",
      "Epoch 264 Batch 0 Loss 0.0067\n",
      "Epoch 264 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16892218589782715 sec\n",
      "\n",
      "Epoch 265 Batch 0 Loss 0.0128\n",
      "Epoch 265 Loss 0.0078\n",
      "Time taken for 1 epoch 0.09099316596984863 sec\n",
      "\n",
      "Epoch 266 Batch 0 Loss 0.0116\n",
      "Epoch 266 Loss 0.0091\n",
      "Time taken for 1 epoch 0.17110824584960938 sec\n",
      "\n",
      "Epoch 267 Batch 0 Loss 0.0025\n",
      "Epoch 267 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08924508094787598 sec\n",
      "\n",
      "Epoch 268 Batch 0 Loss 0.0089\n",
      "Epoch 268 Loss 0.0079\n",
      "Time taken for 1 epoch 0.16555190086364746 sec\n",
      "\n",
      "Epoch 269 Batch 0 Loss 0.0067\n",
      "Epoch 269 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08757591247558594 sec\n",
      "\n",
      "Epoch 270 Batch 0 Loss 0.0049\n",
      "Epoch 270 Loss 0.0068\n",
      "Time taken for 1 epoch 0.1642169952392578 sec\n",
      "\n",
      "Epoch 271 Batch 0 Loss 0.0110\n",
      "Epoch 271 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08636689186096191 sec\n",
      "\n",
      "Epoch 272 Batch 0 Loss 0.0092\n",
      "Epoch 272 Loss 0.0090\n",
      "Time taken for 1 epoch 0.17579197883605957 sec\n",
      "\n",
      "Epoch 273 Batch 0 Loss 0.0112\n",
      "Epoch 273 Loss 0.0079\n",
      "Time taken for 1 epoch 0.09211301803588867 sec\n",
      "\n",
      "Epoch 274 Batch 0 Loss 0.0026\n",
      "Epoch 274 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16736698150634766 sec\n",
      "\n",
      "Epoch 275 Batch 0 Loss 0.0113\n",
      "Epoch 275 Loss 0.0068\n",
      "Time taken for 1 epoch 0.09278297424316406 sec\n",
      "\n",
      "Epoch 276 Batch 0 Loss 0.0089\n",
      "Epoch 276 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16857695579528809 sec\n",
      "\n",
      "Epoch 277 Batch 0 Loss 0.0028\n",
      "Epoch 277 Loss 0.0049\n",
      "Time taken for 1 epoch 0.09058189392089844 sec\n",
      "\n",
      "Epoch 278 Batch 0 Loss 0.0043\n",
      "Epoch 278 Loss 0.0066\n",
      "Time taken for 1 epoch 0.16527104377746582 sec\n",
      "\n",
      "Epoch 279 Batch 0 Loss 0.0003\n",
      "Epoch 279 Loss 0.0066\n",
      "Time taken for 1 epoch 0.08683228492736816 sec\n",
      "\n",
      "Epoch 280 Batch 0 Loss 0.0072\n",
      "Epoch 280 Loss 0.0047\n",
      "Time taken for 1 epoch 0.17053008079528809 sec\n",
      "\n",
      "Epoch 281 Batch 0 Loss 0.0067\n",
      "Epoch 281 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08470582962036133 sec\n",
      "\n",
      "Epoch 282 Batch 0 Loss 0.0085\n",
      "Epoch 282 Loss 0.0091\n",
      "Time taken for 1 epoch 0.16302704811096191 sec\n",
      "\n",
      "Epoch 283 Batch 0 Loss 0.0030\n",
      "Epoch 283 Loss 0.0073\n",
      "Time taken for 1 epoch 0.09047293663024902 sec\n",
      "\n",
      "Epoch 284 Batch 0 Loss 0.0003\n",
      "Epoch 284 Loss 0.0082\n",
      "Time taken for 1 epoch 0.17064499855041504 sec\n",
      "\n",
      "Epoch 285 Batch 0 Loss 0.0039\n",
      "Epoch 285 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08881473541259766 sec\n",
      "\n",
      "Epoch 286 Batch 0 Loss 0.0003\n",
      "Epoch 286 Loss 0.0065\n",
      "Time taken for 1 epoch 0.16489720344543457 sec\n",
      "\n",
      "Epoch 287 Batch 0 Loss 0.0028\n",
      "Epoch 287 Loss 0.0048\n",
      "Time taken for 1 epoch 0.08932685852050781 sec\n",
      "\n",
      "Epoch 288 Batch 0 Loss 0.0094\n",
      "Epoch 288 Loss 0.0059\n",
      "Time taken for 1 epoch 0.16676998138427734 sec\n",
      "\n",
      "Epoch 289 Batch 0 Loss 0.0070\n",
      "Epoch 289 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08924412727355957 sec\n",
      "\n",
      "Epoch 290 Batch 0 Loss 0.0068\n",
      "Epoch 290 Loss 0.0079\n",
      "Time taken for 1 epoch 0.16233491897583008 sec\n",
      "\n",
      "Epoch 291 Batch 0 Loss 0.0112\n",
      "Epoch 291 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08792996406555176 sec\n",
      "\n",
      "Epoch 292 Batch 0 Loss 0.0045\n",
      "Epoch 292 Loss 0.0045\n",
      "Time taken for 1 epoch 0.16400694847106934 sec\n",
      "\n",
      "Epoch 293 Batch 0 Loss 0.0064\n",
      "Epoch 293 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08677029609680176 sec\n",
      "\n",
      "Epoch 294 Batch 0 Loss 0.0025\n",
      "Epoch 294 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16637110710144043 sec\n",
      "\n",
      "Epoch 295 Batch 0 Loss 0.0069\n",
      "Epoch 295 Loss 0.0058\n",
      "Time taken for 1 epoch 0.08829474449157715 sec\n",
      "\n",
      "Epoch 296 Batch 0 Loss 0.0041\n",
      "Epoch 296 Loss 0.0090\n",
      "Time taken for 1 epoch 0.1641690731048584 sec\n",
      "\n",
      "Epoch 297 Batch 0 Loss 0.0041\n",
      "Epoch 297 Loss 0.0054\n",
      "Time taken for 1 epoch 0.08810210227966309 sec\n",
      "\n",
      "Epoch 298 Batch 0 Loss 0.0052\n",
      "Epoch 298 Loss 0.0091\n",
      "Time taken for 1 epoch 0.16372299194335938 sec\n",
      "\n",
      "Epoch 299 Batch 0 Loss 0.0042\n",
      "Epoch 299 Loss 0.0069\n",
      "Time taken for 1 epoch 0.0872650146484375 sec\n",
      "\n",
      "Epoch 300 Batch 0 Loss 0.0114\n",
      "Epoch 300 Loss 0.0067\n",
      "Time taken for 1 epoch 0.1667799949645996 sec\n",
      "\n",
      "Epoch 301 Batch 0 Loss 0.0046\n",
      "Epoch 301 Loss 0.0037\n",
      "Time taken for 1 epoch 0.08846592903137207 sec\n",
      "\n",
      "Epoch 302 Batch 0 Loss 0.0138\n",
      "Epoch 302 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16308021545410156 sec\n",
      "\n",
      "Epoch 303 Batch 0 Loss 0.0104\n",
      "Epoch 303 Loss 0.0078\n",
      "Time taken for 1 epoch 0.0861060619354248 sec\n",
      "\n",
      "Epoch 304 Batch 0 Loss 0.0053\n",
      "Epoch 304 Loss 0.0062\n",
      "Time taken for 1 epoch 0.16440296173095703 sec\n",
      "\n",
      "Epoch 305 Batch 0 Loss 0.0076\n",
      "Epoch 305 Loss 0.0092\n",
      "Time taken for 1 epoch 0.09085702896118164 sec\n",
      "\n",
      "Epoch 306 Batch 0 Loss 0.0139\n",
      "Epoch 306 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16017889976501465 sec\n",
      "\n",
      "Epoch 307 Batch 0 Loss 0.0152\n",
      "Epoch 307 Loss 0.0077\n",
      "Time taken for 1 epoch 0.09766578674316406 sec\n",
      "\n",
      "Epoch 308 Batch 0 Loss 0.0072\n",
      "Epoch 308 Loss 0.0070\n",
      "Time taken for 1 epoch 0.16220784187316895 sec\n",
      "\n",
      "Epoch 309 Batch 0 Loss 0.0114\n",
      "Epoch 309 Loss 0.0082\n",
      "Time taken for 1 epoch 0.08712911605834961 sec\n",
      "\n",
      "Epoch 310 Batch 0 Loss 0.0050\n",
      "Epoch 310 Loss 0.0047\n",
      "Time taken for 1 epoch 0.16551709175109863 sec\n",
      "\n",
      "Epoch 311 Batch 0 Loss 0.0023\n",
      "Epoch 311 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08852982521057129 sec\n",
      "\n",
      "Epoch 312 Batch 0 Loss 0.0114\n",
      "Epoch 312 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16643309593200684 sec\n",
      "\n",
      "Epoch 313 Batch 0 Loss 0.0023\n",
      "Epoch 313 Loss 0.0032\n",
      "Time taken for 1 epoch 0.08845376968383789 sec\n",
      "\n",
      "Epoch 314 Batch 0 Loss 0.0044\n",
      "Epoch 314 Loss 0.0084\n",
      "Time taken for 1 epoch 0.1629009246826172 sec\n",
      "\n",
      "Epoch 315 Batch 0 Loss 0.0059\n",
      "Epoch 315 Loss 0.0091\n",
      "Time taken for 1 epoch 0.08821296691894531 sec\n",
      "\n",
      "Epoch 316 Batch 0 Loss 0.0095\n",
      "Epoch 316 Loss 0.0069\n",
      "Time taken for 1 epoch 0.16389894485473633 sec\n",
      "\n",
      "Epoch 317 Batch 0 Loss 0.0076\n",
      "Epoch 317 Loss 0.0061\n",
      "Time taken for 1 epoch 0.0876150131225586 sec\n",
      "\n",
      "Epoch 318 Batch 0 Loss 0.0115\n",
      "Epoch 318 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16675186157226562 sec\n",
      "\n",
      "Epoch 319 Batch 0 Loss 0.0051\n",
      "Epoch 319 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08735799789428711 sec\n",
      "\n",
      "Epoch 320 Batch 0 Loss 0.0073\n",
      "Epoch 320 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16733598709106445 sec\n",
      "\n",
      "Epoch 321 Batch 0 Loss 0.0046\n",
      "Epoch 321 Loss 0.0057\n",
      "Time taken for 1 epoch 0.0867912769317627 sec\n",
      "\n",
      "Epoch 322 Batch 0 Loss 0.0093\n",
      "Epoch 322 Loss 0.0090\n",
      "Time taken for 1 epoch 0.1653590202331543 sec\n",
      "\n",
      "Epoch 323 Batch 0 Loss 0.0027\n",
      "Epoch 323 Loss 0.0058\n",
      "Time taken for 1 epoch 0.08820796012878418 sec\n",
      "\n",
      "Epoch 324 Batch 0 Loss 0.0089\n",
      "Epoch 324 Loss 0.0069\n",
      "Time taken for 1 epoch 0.16541218757629395 sec\n",
      "\n",
      "Epoch 325 Batch 0 Loss 0.0088\n",
      "Epoch 325 Loss 0.0065\n",
      "Time taken for 1 epoch 0.08670783042907715 sec\n",
      "\n",
      "Epoch 326 Batch 0 Loss 0.0108\n",
      "Epoch 326 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16310667991638184 sec\n",
      "\n",
      "Epoch 327 Batch 0 Loss 0.0048\n",
      "Epoch 327 Loss 0.0060\n",
      "Time taken for 1 epoch 0.09064102172851562 sec\n",
      "\n",
      "Epoch 328 Batch 0 Loss 0.0115\n",
      "Epoch 328 Loss 0.0077\n",
      "Time taken for 1 epoch 0.1619272232055664 sec\n",
      "\n",
      "Epoch 329 Batch 0 Loss 0.0089\n",
      "Epoch 329 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08643984794616699 sec\n",
      "\n",
      "Epoch 330 Batch 0 Loss 0.0066\n",
      "Epoch 330 Loss 0.0044\n",
      "Time taken for 1 epoch 0.15890097618103027 sec\n",
      "\n",
      "Epoch 331 Batch 0 Loss 0.0093\n",
      "Epoch 331 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08652901649475098 sec\n",
      "\n",
      "Epoch 332 Batch 0 Loss 0.0086\n",
      "Epoch 332 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16202974319458008 sec\n",
      "\n",
      "Epoch 333 Batch 0 Loss 0.0069\n",
      "Epoch 333 Loss 0.0057\n",
      "Time taken for 1 epoch 0.08815383911132812 sec\n",
      "\n",
      "Epoch 334 Batch 0 Loss 0.0023\n",
      "Epoch 334 Loss 0.0080\n",
      "Time taken for 1 epoch 0.17062711715698242 sec\n",
      "\n",
      "Epoch 335 Batch 0 Loss 0.0090\n",
      "Epoch 335 Loss 0.0057\n",
      "Time taken for 1 epoch 0.08640098571777344 sec\n",
      "\n",
      "Epoch 336 Batch 0 Loss 0.0133\n",
      "Epoch 336 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16443300247192383 sec\n",
      "\n",
      "Epoch 337 Batch 0 Loss 0.0021\n",
      "Epoch 337 Loss 0.0080\n",
      "Time taken for 1 epoch 0.0920112133026123 sec\n",
      "\n",
      "Epoch 338 Batch 0 Loss 0.0066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 338 Loss 0.0081\n",
      "Time taken for 1 epoch 0.16122126579284668 sec\n",
      "\n",
      "Epoch 339 Batch 0 Loss 0.0072\n",
      "Epoch 339 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08742713928222656 sec\n",
      "\n",
      "Epoch 340 Batch 0 Loss 0.0067\n",
      "Epoch 340 Loss 0.0078\n",
      "Time taken for 1 epoch 0.15874004364013672 sec\n",
      "\n",
      "Epoch 341 Batch 0 Loss 0.0065\n",
      "Epoch 341 Loss 0.0067\n",
      "Time taken for 1 epoch 0.08835005760192871 sec\n",
      "\n",
      "Epoch 342 Batch 0 Loss 0.0022\n",
      "Epoch 342 Loss 0.0066\n",
      "Time taken for 1 epoch 0.16133975982666016 sec\n",
      "\n",
      "Epoch 343 Batch 0 Loss 0.0110\n",
      "Epoch 343 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08833694458007812 sec\n",
      "\n",
      "Epoch 344 Batch 0 Loss 0.0109\n",
      "Epoch 344 Loss 0.0069\n",
      "Time taken for 1 epoch 0.1609022617340088 sec\n",
      "\n",
      "Epoch 345 Batch 0 Loss 0.0135\n",
      "Epoch 345 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08762335777282715 sec\n",
      "\n",
      "Epoch 346 Batch 0 Loss 0.0064\n",
      "Epoch 346 Loss 0.0057\n",
      "Time taken for 1 epoch 0.15897226333618164 sec\n",
      "\n",
      "Epoch 347 Batch 0 Loss 0.0132\n",
      "Epoch 347 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08631110191345215 sec\n",
      "\n",
      "Epoch 348 Batch 0 Loss 0.0047\n",
      "Epoch 348 Loss 0.0069\n",
      "Time taken for 1 epoch 0.15941572189331055 sec\n",
      "\n",
      "Epoch 349 Batch 0 Loss 0.0073\n",
      "Epoch 349 Loss 0.0077\n",
      "Time taken for 1 epoch 0.0850827693939209 sec\n",
      "\n",
      "Epoch 350 Batch 0 Loss 0.0134\n",
      "Epoch 350 Loss 0.0068\n",
      "Time taken for 1 epoch 0.1598658561706543 sec\n",
      "\n",
      "Epoch 351 Batch 0 Loss 0.0068\n",
      "Epoch 351 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08751797676086426 sec\n",
      "\n",
      "Epoch 352 Batch 0 Loss 0.0047\n",
      "Epoch 352 Loss 0.0065\n",
      "Time taken for 1 epoch 0.1621391773223877 sec\n",
      "\n",
      "Epoch 353 Batch 0 Loss 0.0099\n",
      "Epoch 353 Loss 0.0072\n",
      "Time taken for 1 epoch 0.08655309677124023 sec\n",
      "\n",
      "Epoch 354 Batch 0 Loss 0.0096\n",
      "Epoch 354 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16268014907836914 sec\n",
      "\n",
      "Epoch 355 Batch 0 Loss 0.0114\n",
      "Epoch 355 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08538985252380371 sec\n",
      "\n",
      "Epoch 356 Batch 0 Loss 0.0021\n",
      "Epoch 356 Loss 0.0067\n",
      "Time taken for 1 epoch 0.16000103950500488 sec\n",
      "\n",
      "Epoch 357 Batch 0 Loss 0.0139\n",
      "Epoch 357 Loss 0.0081\n",
      "Time taken for 1 epoch 0.08716273307800293 sec\n",
      "\n",
      "Epoch 358 Batch 0 Loss 0.0064\n",
      "Epoch 358 Loss 0.0055\n",
      "Time taken for 1 epoch 0.1597728729248047 sec\n",
      "\n",
      "Epoch 359 Batch 0 Loss 0.0044\n",
      "Epoch 359 Loss 0.0055\n",
      "Time taken for 1 epoch 0.08754110336303711 sec\n",
      "\n",
      "Epoch 360 Batch 0 Loss 0.0068\n",
      "Epoch 360 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16019725799560547 sec\n",
      "\n",
      "Epoch 361 Batch 0 Loss 0.0089\n",
      "Epoch 361 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08910298347473145 sec\n",
      "\n",
      "Epoch 362 Batch 0 Loss 0.0157\n",
      "Epoch 362 Loss 0.0080\n",
      "Time taken for 1 epoch 0.160383939743042 sec\n",
      "\n",
      "Epoch 363 Batch 0 Loss 0.0049\n",
      "Epoch 363 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08733510971069336 sec\n",
      "\n",
      "Epoch 364 Batch 0 Loss 0.0108\n",
      "Epoch 364 Loss 0.0078\n",
      "Time taken for 1 epoch 0.1601710319519043 sec\n",
      "\n",
      "Epoch 365 Batch 0 Loss 0.0066\n",
      "Epoch 365 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08657264709472656 sec\n",
      "\n",
      "Epoch 366 Batch 0 Loss 0.0064\n",
      "Epoch 366 Loss 0.0077\n",
      "Time taken for 1 epoch 0.16915583610534668 sec\n",
      "\n",
      "Epoch 367 Batch 0 Loss 0.0053\n",
      "Epoch 367 Loss 0.0038\n",
      "Time taken for 1 epoch 0.08825993537902832 sec\n",
      "\n",
      "Epoch 368 Batch 0 Loss 0.0107\n",
      "Epoch 368 Loss 0.0078\n",
      "Time taken for 1 epoch 0.15964078903198242 sec\n",
      "\n",
      "Epoch 369 Batch 0 Loss 0.0046\n",
      "Epoch 369 Loss 0.0081\n",
      "Time taken for 1 epoch 0.08888888359069824 sec\n",
      "\n",
      "Epoch 370 Batch 0 Loss 0.0064\n",
      "Epoch 370 Loss 0.0064\n",
      "Time taken for 1 epoch 0.1631922721862793 sec\n",
      "\n",
      "Epoch 371 Batch 0 Loss 0.0067\n",
      "Epoch 371 Loss 0.0068\n",
      "Time taken for 1 epoch 0.0895071029663086 sec\n",
      "\n",
      "Epoch 372 Batch 0 Loss 0.0088\n",
      "Epoch 372 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16388416290283203 sec\n",
      "\n",
      "Epoch 373 Batch 0 Loss 0.0066\n",
      "Epoch 373 Loss 0.0034\n",
      "Time taken for 1 epoch 0.08859992027282715 sec\n",
      "\n",
      "Epoch 374 Batch 0 Loss 0.0092\n",
      "Epoch 374 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16653728485107422 sec\n",
      "\n",
      "Epoch 375 Batch 0 Loss 0.0103\n",
      "Epoch 375 Loss 0.0066\n",
      "Time taken for 1 epoch 0.08793497085571289 sec\n",
      "\n",
      "Epoch 376 Batch 0 Loss 0.0002\n",
      "Epoch 376 Loss 0.0064\n",
      "Time taken for 1 epoch 0.16757702827453613 sec\n",
      "\n",
      "Epoch 377 Batch 0 Loss 0.0090\n",
      "Epoch 377 Loss 0.0068\n",
      "Time taken for 1 epoch 0.09038519859313965 sec\n",
      "\n",
      "Epoch 378 Batch 0 Loss 0.0073\n",
      "Epoch 378 Loss 0.0069\n",
      "Time taken for 1 epoch 0.16235804557800293 sec\n",
      "\n",
      "Epoch 379 Batch 0 Loss 0.0091\n",
      "Epoch 379 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08849668502807617 sec\n",
      "\n",
      "Epoch 380 Batch 0 Loss 0.0047\n",
      "Epoch 380 Loss 0.0055\n",
      "Time taken for 1 epoch 0.16299009323120117 sec\n",
      "\n",
      "Epoch 381 Batch 0 Loss 0.0089\n",
      "Epoch 381 Loss 0.0091\n",
      "Time taken for 1 epoch 0.09112000465393066 sec\n",
      "\n",
      "Epoch 382 Batch 0 Loss 0.0109\n",
      "Epoch 382 Loss 0.0069\n",
      "Time taken for 1 epoch 0.1656801700592041 sec\n",
      "\n",
      "Epoch 383 Batch 0 Loss 0.0072\n",
      "Epoch 383 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08993911743164062 sec\n",
      "\n",
      "Epoch 384 Batch 0 Loss 0.0042\n",
      "Epoch 384 Loss 0.0061\n",
      "Time taken for 1 epoch 0.1646568775177002 sec\n",
      "\n",
      "Epoch 385 Batch 0 Loss 0.0130\n",
      "Epoch 385 Loss 0.0090\n",
      "Time taken for 1 epoch 0.09039998054504395 sec\n",
      "\n",
      "Epoch 386 Batch 0 Loss 0.0068\n",
      "Epoch 386 Loss 0.0047\n",
      "Time taken for 1 epoch 0.1658649444580078 sec\n",
      "\n",
      "Epoch 387 Batch 0 Loss 0.0046\n",
      "Epoch 387 Loss 0.0078\n",
      "Time taken for 1 epoch 0.0896139144897461 sec\n",
      "\n",
      "Epoch 388 Batch 0 Loss 0.0070\n",
      "Epoch 388 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16788578033447266 sec\n",
      "\n",
      "Epoch 389 Batch 0 Loss 0.0069\n",
      "Epoch 389 Loss 0.0057\n",
      "Time taken for 1 epoch 0.08717489242553711 sec\n",
      "\n",
      "Epoch 390 Batch 0 Loss 0.0087\n",
      "Epoch 390 Loss 0.0078\n",
      "Time taken for 1 epoch 0.16682887077331543 sec\n",
      "\n",
      "Epoch 391 Batch 0 Loss 0.0088\n",
      "Epoch 391 Loss 0.0057\n",
      "Time taken for 1 epoch 0.08742284774780273 sec\n",
      "\n",
      "Epoch 392 Batch 0 Loss 0.0156\n",
      "Epoch 392 Loss 0.0089\n",
      "Time taken for 1 epoch 0.16486191749572754 sec\n",
      "\n",
      "Epoch 393 Batch 0 Loss 0.0072\n",
      "Epoch 393 Loss 0.0047\n",
      "Time taken for 1 epoch 0.08897995948791504 sec\n",
      "\n",
      "Epoch 394 Batch 0 Loss 0.0067\n",
      "Epoch 394 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16667509078979492 sec\n",
      "\n",
      "Epoch 395 Batch 0 Loss 0.0086\n",
      "Epoch 395 Loss 0.0055\n",
      "Time taken for 1 epoch 0.08870100975036621 sec\n",
      "\n",
      "Epoch 396 Batch 0 Loss 0.0064\n",
      "Epoch 396 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16259479522705078 sec\n",
      "\n",
      "Epoch 397 Batch 0 Loss 0.0045\n",
      "Epoch 397 Loss 0.0078\n",
      "Time taken for 1 epoch 0.0875399112701416 sec\n",
      "\n",
      "Epoch 398 Batch 0 Loss 0.0089\n",
      "Epoch 398 Loss 0.0065\n",
      "Time taken for 1 epoch 0.16535282135009766 sec\n",
      "\n",
      "Epoch 399 Batch 0 Loss 0.0111\n",
      "Epoch 399 Loss 0.0079\n",
      "Time taken for 1 epoch 0.0885927677154541 sec\n",
      "\n",
      "Epoch 400 Batch 0 Loss 0.0073\n",
      "Epoch 400 Loss 0.0081\n",
      "Time taken for 1 epoch 0.1633899211883545 sec\n",
      "\n",
      "Epoch 401 Batch 0 Loss 0.0084\n",
      "Epoch 401 Loss 0.0056\n",
      "Time taken for 1 epoch 0.08921194076538086 sec\n",
      "\n",
      "Epoch 402 Batch 0 Loss 0.0109\n",
      "Epoch 402 Loss 0.0082\n",
      "Time taken for 1 epoch 0.1643970012664795 sec\n",
      "\n",
      "Epoch 403 Batch 0 Loss 0.0086\n",
      "Epoch 403 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08771109580993652 sec\n",
      "\n",
      "Epoch 404 Batch 0 Loss 0.0070\n",
      "Epoch 404 Loss 0.0057\n",
      "Time taken for 1 epoch 0.16064000129699707 sec\n",
      "\n",
      "Epoch 405 Batch 0 Loss 0.0088\n",
      "Epoch 405 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08803009986877441 sec\n",
      "\n",
      "Epoch 406 Batch 0 Loss 0.0042\n",
      "Epoch 406 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16554784774780273 sec\n",
      "\n",
      "Epoch 407 Batch 0 Loss 0.0043\n",
      "Epoch 407 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08705472946166992 sec\n",
      "\n",
      "Epoch 408 Batch 0 Loss 0.0093\n",
      "Epoch 408 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16238808631896973 sec\n",
      "\n",
      "Epoch 409 Batch 0 Loss 0.0052\n",
      "Epoch 409 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08966803550720215 sec\n",
      "\n",
      "Epoch 410 Batch 0 Loss 0.0043\n",
      "Epoch 410 Loss 0.0078\n",
      "Time taken for 1 epoch 0.1674811840057373 sec\n",
      "\n",
      "Epoch 411 Batch 0 Loss 0.0070\n",
      "Epoch 411 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08832836151123047 sec\n",
      "\n",
      "Epoch 412 Batch 0 Loss 0.0064\n",
      "Epoch 412 Loss 0.0055\n",
      "Time taken for 1 epoch 0.1614851951599121 sec\n",
      "\n",
      "Epoch 413 Batch 0 Loss 0.0106\n",
      "Epoch 413 Loss 0.0080\n",
      "Time taken for 1 epoch 0.08795714378356934 sec\n",
      "\n",
      "Epoch 414 Batch 0 Loss 0.0065\n",
      "Epoch 414 Loss 0.0045\n",
      "Time taken for 1 epoch 0.1636507511138916 sec\n",
      "\n",
      "Epoch 415 Batch 0 Loss 0.0070\n",
      "Epoch 415 Loss 0.0090\n",
      "Time taken for 1 epoch 0.11630105972290039 sec\n",
      "\n",
      "Epoch 416 Batch 0 Loss 0.0048\n",
      "Epoch 416 Loss 0.0080\n",
      "Time taken for 1 epoch 0.1831531524658203 sec\n",
      "\n",
      "Epoch 417 Batch 0 Loss 0.0072\n",
      "Epoch 417 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08893394470214844 sec\n",
      "\n",
      "Epoch 418 Batch 0 Loss 0.0072\n",
      "Epoch 418 Loss 0.0070\n",
      "Time taken for 1 epoch 0.16409611701965332 sec\n",
      "\n",
      "Epoch 419 Batch 0 Loss 0.0089\n",
      "Epoch 419 Loss 0.0077\n",
      "Time taken for 1 epoch 0.08674001693725586 sec\n",
      "\n",
      "Epoch 420 Batch 0 Loss 0.0018\n",
      "Epoch 420 Loss 0.0078\n",
      "Time taken for 1 epoch 0.16506195068359375 sec\n",
      "\n",
      "Epoch 421 Batch 0 Loss 0.0119\n",
      "Epoch 421 Loss 0.0082\n",
      "Time taken for 1 epoch 0.09049010276794434 sec\n",
      "\n",
      "Epoch 422 Batch 0 Loss 0.0046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 422 Loss 0.0063\n",
      "Time taken for 1 epoch 0.16673612594604492 sec\n",
      "\n",
      "Epoch 423 Batch 0 Loss 0.0091\n",
      "Epoch 423 Loss 0.0056\n",
      "Time taken for 1 epoch 0.08894991874694824 sec\n",
      "\n",
      "Epoch 424 Batch 0 Loss 0.0098\n",
      "Epoch 424 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16266226768493652 sec\n",
      "\n",
      "Epoch 425 Batch 0 Loss 0.0129\n",
      "Epoch 425 Loss 0.0066\n",
      "Time taken for 1 epoch 0.08823704719543457 sec\n",
      "\n",
      "Epoch 426 Batch 0 Loss 0.0078\n",
      "Epoch 426 Loss 0.0062\n",
      "Time taken for 1 epoch 0.16590499877929688 sec\n",
      "\n",
      "Epoch 427 Batch 0 Loss 0.0072\n",
      "Epoch 427 Loss 0.0075\n",
      "Time taken for 1 epoch 0.08843016624450684 sec\n",
      "\n",
      "Epoch 428 Batch 0 Loss 0.0040\n",
      "Epoch 428 Loss 0.0052\n",
      "Time taken for 1 epoch 0.16344118118286133 sec\n",
      "\n",
      "Epoch 429 Batch 0 Loss 0.0027\n",
      "Epoch 429 Loss 0.0081\n",
      "Time taken for 1 epoch 0.08883118629455566 sec\n",
      "\n",
      "Epoch 430 Batch 0 Loss 0.0054\n",
      "Epoch 430 Loss 0.0079\n",
      "Time taken for 1 epoch 0.16922712326049805 sec\n",
      "\n",
      "Epoch 431 Batch 0 Loss 0.0045\n",
      "Epoch 431 Loss 0.0065\n",
      "Time taken for 1 epoch 0.08499288558959961 sec\n",
      "\n",
      "Epoch 432 Batch 0 Loss 0.0045\n",
      "Epoch 432 Loss 0.0081\n",
      "Time taken for 1 epoch 0.1658933162689209 sec\n",
      "\n",
      "Epoch 433 Batch 0 Loss 0.0122\n",
      "Epoch 433 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08807015419006348 sec\n",
      "\n",
      "Epoch 434 Batch 0 Loss 0.0065\n",
      "Epoch 434 Loss 0.0053\n",
      "Time taken for 1 epoch 0.16373205184936523 sec\n",
      "\n",
      "Epoch 435 Batch 0 Loss 0.0002\n",
      "Epoch 435 Loss 0.0042\n",
      "Time taken for 1 epoch 0.08675193786621094 sec\n",
      "\n",
      "Epoch 436 Batch 0 Loss 0.0046\n",
      "Epoch 436 Loss 0.0080\n",
      "Time taken for 1 epoch 0.17449116706848145 sec\n",
      "\n",
      "Epoch 437 Batch 0 Loss 0.0068\n",
      "Epoch 437 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08805704116821289 sec\n",
      "\n",
      "Epoch 438 Batch 0 Loss 0.0163\n",
      "Epoch 438 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16283178329467773 sec\n",
      "\n",
      "Epoch 439 Batch 0 Loss 0.0069\n",
      "Epoch 439 Loss 0.0082\n",
      "Time taken for 1 epoch 0.08571910858154297 sec\n",
      "\n",
      "Epoch 440 Batch 0 Loss 0.0062\n",
      "Epoch 440 Loss 0.0054\n",
      "Time taken for 1 epoch 0.16701889038085938 sec\n",
      "\n",
      "Epoch 441 Batch 0 Loss 0.0110\n",
      "Epoch 441 Loss 0.0081\n",
      "Time taken for 1 epoch 0.09048223495483398 sec\n",
      "\n",
      "Epoch 442 Batch 0 Loss 0.0131\n",
      "Epoch 442 Loss 0.0089\n",
      "Time taken for 1 epoch 0.16533517837524414 sec\n",
      "\n",
      "Epoch 443 Batch 0 Loss 0.0070\n",
      "Epoch 443 Loss 0.0047\n",
      "Time taken for 1 epoch 0.08839011192321777 sec\n",
      "\n",
      "Epoch 444 Batch 0 Loss 0.0089\n",
      "Epoch 444 Loss 0.0057\n",
      "Time taken for 1 epoch 0.16439104080200195 sec\n",
      "\n",
      "Epoch 445 Batch 0 Loss 0.0132\n",
      "Epoch 445 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08991694450378418 sec\n",
      "\n",
      "Epoch 446 Batch 0 Loss 0.0068\n",
      "Epoch 446 Loss 0.0069\n",
      "Time taken for 1 epoch 0.17105698585510254 sec\n",
      "\n",
      "Epoch 447 Batch 0 Loss 0.0110\n",
      "Epoch 447 Loss 0.0068\n",
      "Time taken for 1 epoch 0.08842802047729492 sec\n",
      "\n",
      "Epoch 448 Batch 0 Loss 0.0115\n",
      "Epoch 448 Loss 0.0079\n",
      "Time taken for 1 epoch 0.16671371459960938 sec\n",
      "\n",
      "Epoch 449 Batch 0 Loss 0.0024\n",
      "Epoch 449 Loss 0.0066\n",
      "Time taken for 1 epoch 0.09085822105407715 sec\n",
      "\n",
      "Epoch 450 Batch 0 Loss 0.0154\n",
      "Epoch 450 Loss 0.0078\n",
      "Time taken for 1 epoch 0.1651918888092041 sec\n",
      "\n",
      "Epoch 451 Batch 0 Loss 0.0052\n",
      "Epoch 451 Loss 0.0046\n",
      "Time taken for 1 epoch 0.08713698387145996 sec\n",
      "\n",
      "Epoch 452 Batch 0 Loss 0.0096\n",
      "Epoch 452 Loss 0.0079\n",
      "Time taken for 1 epoch 0.16240906715393066 sec\n",
      "\n",
      "Epoch 453 Batch 0 Loss 0.0112\n",
      "Epoch 453 Loss 0.0057\n",
      "Time taken for 1 epoch 0.08784198760986328 sec\n",
      "\n",
      "Epoch 454 Batch 0 Loss 0.0131\n",
      "Epoch 454 Loss 0.0090\n",
      "Time taken for 1 epoch 0.1666889190673828 sec\n",
      "\n",
      "Epoch 455 Batch 0 Loss 0.0024\n",
      "Epoch 455 Loss 0.0037\n",
      "Time taken for 1 epoch 0.08952188491821289 sec\n",
      "\n",
      "Epoch 456 Batch 0 Loss 0.0025\n",
      "Epoch 456 Loss 0.0056\n",
      "Time taken for 1 epoch 0.16741681098937988 sec\n",
      "\n",
      "Epoch 457 Batch 0 Loss 0.0128\n",
      "Epoch 457 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08894610404968262 sec\n",
      "\n",
      "Epoch 458 Batch 0 Loss 0.0067\n",
      "Epoch 458 Loss 0.0049\n",
      "Time taken for 1 epoch 0.1656200885772705 sec\n",
      "\n",
      "Epoch 459 Batch 0 Loss 0.0046\n",
      "Epoch 459 Loss 0.0064\n",
      "Time taken for 1 epoch 0.0867760181427002 sec\n",
      "\n",
      "Epoch 460 Batch 0 Loss 0.0068\n",
      "Epoch 460 Loss 0.0068\n",
      "Time taken for 1 epoch 0.1671302318572998 sec\n",
      "\n",
      "Epoch 461 Batch 0 Loss 0.0154\n",
      "Epoch 461 Loss 0.0090\n",
      "Time taken for 1 epoch 0.08871197700500488 sec\n",
      "\n",
      "Epoch 462 Batch 0 Loss 0.0041\n",
      "Epoch 462 Loss 0.0057\n",
      "Time taken for 1 epoch 0.16756892204284668 sec\n",
      "\n",
      "Epoch 463 Batch 0 Loss 0.0088\n",
      "Epoch 463 Loss 0.0058\n",
      "Time taken for 1 epoch 0.0865018367767334 sec\n",
      "\n",
      "Epoch 464 Batch 0 Loss 0.0026\n",
      "Epoch 464 Loss 0.0090\n",
      "Time taken for 1 epoch 0.1631619930267334 sec\n",
      "\n",
      "Epoch 465 Batch 0 Loss 0.0154\n",
      "Epoch 465 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08951807022094727 sec\n",
      "\n",
      "Epoch 466 Batch 0 Loss 0.0026\n",
      "Epoch 466 Loss 0.0058\n",
      "Time taken for 1 epoch 0.16469097137451172 sec\n",
      "\n",
      "Epoch 467 Batch 0 Loss 0.0094\n",
      "Epoch 467 Loss 0.0078\n",
      "Time taken for 1 epoch 0.09239792823791504 sec\n",
      "\n",
      "Epoch 468 Batch 0 Loss 0.0070\n",
      "Epoch 468 Loss 0.0080\n",
      "Time taken for 1 epoch 0.16567325592041016 sec\n",
      "\n",
      "Epoch 469 Batch 0 Loss 0.0024\n",
      "Epoch 469 Loss 0.0058\n",
      "Time taken for 1 epoch 0.08921194076538086 sec\n",
      "\n",
      "Epoch 470 Batch 0 Loss 0.0086\n",
      "Epoch 470 Loss 0.0056\n",
      "Time taken for 1 epoch 0.16336703300476074 sec\n",
      "\n",
      "Epoch 471 Batch 0 Loss 0.0026\n",
      "Epoch 471 Loss 0.0066\n",
      "Time taken for 1 epoch 0.08939790725708008 sec\n",
      "\n",
      "Epoch 472 Batch 0 Loss 0.0070\n",
      "Epoch 472 Loss 0.0089\n",
      "Time taken for 1 epoch 0.16771793365478516 sec\n",
      "\n",
      "Epoch 473 Batch 0 Loss 0.0040\n",
      "Epoch 473 Loss 0.0046\n",
      "Time taken for 1 epoch 0.08783578872680664 sec\n",
      "\n",
      "Epoch 474 Batch 0 Loss 0.0117\n",
      "Epoch 474 Loss 0.0090\n",
      "Time taken for 1 epoch 0.1657102108001709 sec\n",
      "\n",
      "Epoch 475 Batch 0 Loss 0.0071\n",
      "Epoch 475 Loss 0.0090\n",
      "Time taken for 1 epoch 0.0894930362701416 sec\n",
      "\n",
      "Epoch 476 Batch 0 Loss 0.0092\n",
      "Epoch 476 Loss 0.0077\n",
      "Time taken for 1 epoch 0.16639995574951172 sec\n",
      "\n",
      "Epoch 477 Batch 0 Loss 0.0115\n",
      "Epoch 477 Loss 0.0077\n",
      "Time taken for 1 epoch 0.08855915069580078 sec\n",
      "\n",
      "Epoch 478 Batch 0 Loss 0.0047\n",
      "Epoch 478 Loss 0.0076\n",
      "Time taken for 1 epoch 0.16652822494506836 sec\n",
      "\n",
      "Epoch 479 Batch 0 Loss 0.0046\n",
      "Epoch 479 Loss 0.0075\n",
      "Time taken for 1 epoch 0.08966302871704102 sec\n",
      "\n",
      "Epoch 480 Batch 0 Loss 0.0040\n",
      "Epoch 480 Loss 0.0069\n",
      "Time taken for 1 epoch 0.16398906707763672 sec\n",
      "\n",
      "Epoch 481 Batch 0 Loss 0.0101\n",
      "Epoch 481 Loss 0.0091\n",
      "Time taken for 1 epoch 0.08896160125732422 sec\n",
      "\n",
      "Epoch 482 Batch 0 Loss 0.0104\n",
      "Epoch 482 Loss 0.0091\n",
      "Time taken for 1 epoch 0.16878509521484375 sec\n",
      "\n",
      "Epoch 483 Batch 0 Loss 0.0040\n",
      "Epoch 483 Loss 0.0069\n",
      "Time taken for 1 epoch 0.08581900596618652 sec\n",
      "\n",
      "Epoch 484 Batch 0 Loss 0.0045\n",
      "Epoch 484 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16696381568908691 sec\n",
      "\n",
      "Epoch 485 Batch 0 Loss 0.0089\n",
      "Epoch 485 Loss 0.0074\n",
      "Time taken for 1 epoch 0.09014701843261719 sec\n",
      "\n",
      "Epoch 486 Batch 0 Loss 0.0024\n",
      "Epoch 486 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16577625274658203 sec\n",
      "\n",
      "Epoch 487 Batch 0 Loss 0.0038\n",
      "Epoch 487 Loss 0.0068\n",
      "Time taken for 1 epoch 0.0917351245880127 sec\n",
      "\n",
      "Epoch 488 Batch 0 Loss 0.0113\n",
      "Epoch 488 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16835618019104004 sec\n",
      "\n",
      "Epoch 489 Batch 0 Loss 0.0031\n",
      "Epoch 489 Loss 0.0079\n",
      "Time taken for 1 epoch 0.08812499046325684 sec\n",
      "\n",
      "Epoch 490 Batch 0 Loss 0.0132\n",
      "Epoch 490 Loss 0.0090\n",
      "Time taken for 1 epoch 0.16982078552246094 sec\n",
      "\n",
      "Epoch 491 Batch 0 Loss 0.0047\n",
      "Epoch 491 Loss 0.0046\n",
      "Time taken for 1 epoch 0.09142708778381348 sec\n",
      "\n",
      "Epoch 492 Batch 0 Loss 0.0089\n",
      "Epoch 492 Loss 0.0068\n",
      "Time taken for 1 epoch 0.16683387756347656 sec\n",
      "\n",
      "Epoch 493 Batch 0 Loss 0.0070\n",
      "Epoch 493 Loss 0.0078\n",
      "Time taken for 1 epoch 0.08852910995483398 sec\n",
      "\n",
      "Epoch 494 Batch 0 Loss 0.0045\n",
      "Epoch 494 Loss 0.0045\n",
      "Time taken for 1 epoch 0.16570591926574707 sec\n",
      "\n",
      "Epoch 495 Batch 0 Loss 0.0022\n",
      "Epoch 495 Loss 0.0045\n",
      "Time taken for 1 epoch 0.0894327163696289 sec\n",
      "\n",
      "Epoch 496 Batch 0 Loss 0.0112\n",
      "Epoch 496 Loss 0.0077\n",
      "Time taken for 1 epoch 0.16618871688842773 sec\n",
      "\n",
      "Epoch 497 Batch 0 Loss 0.0087\n",
      "Epoch 497 Loss 0.0077\n",
      "Time taken for 1 epoch 0.08692407608032227 sec\n",
      "\n",
      "Epoch 498 Batch 0 Loss 0.0113\n",
      "Epoch 498 Loss 0.0076\n",
      "Time taken for 1 epoch 0.1679680347442627 sec\n",
      "\n",
      "Epoch 499 Batch 0 Loss 0.0046\n",
      "Epoch 499 Loss 0.0055\n",
      "Time taken for 1 epoch 0.0874168872833252 sec\n",
      "\n",
      "Epoch 500 Batch 0 Loss 0.0031\n",
      "Epoch 500 Loss 0.0060\n",
      "Time taken for 1 epoch 0.16393089294433594 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 500 # specifying the number of epochs or runs for training the model\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  enc_hidden = model.layers[0].initialize_states(64)\n",
    "  total_loss = 0\n",
    "\n",
    "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "    batch_loss = train_step(inp, targ, enc_hidden)\n",
    "    total_loss += batch_loss\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "      \n",
    "  if (epoch + 1) % 2 == 0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q tensorflow-recommenders\n",
    "# !pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the keras model\n",
    "import tensorflow_recommenders as tfrs\n",
    "model.retrieval_task = tfrs.tasks.Retrieval()  # Removes the metrics.\n",
    "model.compile()\n",
    "tf.saved_model.save(model, '/Users/cm15/Desktop')\n",
    "\n",
    "# Loading the Keras Model\n",
    "# model = tf.saved_model.load(path_to_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9ZczUDcVG9B"
   },
   "source": [
    "### Translate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Excafr5LU_9p"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence):\n",
    "\n",
    "  attention_plot = np.zeros((output_len, input_len))\n",
    "\n",
    "  input_sentence = preprocess_sentence(input_sentence)\n",
    "\n",
    "  inputs = [inp_lang.word_index[i] for i in input_sentence.split()]\n",
    "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=input_len,\n",
    "                                                         padding='post')\n",
    "  inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "  result = ''\n",
    "  \n",
    "  encoder_output,state_h,state_c = model.layers[0](inputs,[tf.zeros((1, lstm_size)),tf.zeros((1, lstm_size))])\n",
    "\n",
    "  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "  for t in range(output_len):\n",
    "   predictions,state_h,state_c,attention_weights,context_vector = model.layers[1].onestepdecoder(dec_input,\n",
    "                                                                                                 encoder_output,\n",
    "                                                                                                 state_h,\n",
    "                                                                                                 state_c)\n",
    "\n",
    "   attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "   attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "   predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "   result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "   if targ_lang.index_word[predicted_id] == '<end>':\n",
    "     return result, input_sentence, attention_plot\n",
    "\n",
    "   dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "  return result, input_sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "O-U2_Lb-VObw"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sent, attention_plot = predict(sentence)\n",
    "\n",
    "  print('Input: %s' % (sent))\n",
    "  print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3sl-j0yVQax",
    "outputId": "b720ce20-9143-40c6-b171-12e80a490341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> to agree  <end>\n",
      "Predicted translation: bvomera . <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"to agree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sent, attention_plot = predict(sentence)\n",
    "\n",
    "  return result\n",
    "UI = gd.Interface(translate, inputs='text', outputs='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x17749ee90>, 'http://127.0.0.1:7860/', None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UI.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Machine Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
